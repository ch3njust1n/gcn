{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "1. Softmax layer\n",
    "2. Fix backprop\n",
    "3. Gradient checking\n",
    "4. Visualize loss\n",
    "5. Embeddings\n",
    "6. Gradient of cross ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import argparse\n",
    "import random\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from pyvis.network import Network\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.algorithms.community.modularity_max import greedy_modularity_communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 100\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.karate_club_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34, 78)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.number_of_nodes(), G.number_of_edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate labels from communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "communities = greedy_modularity_communities(G)\n",
    "colors = np.zeros(G.number_of_nodes())\n",
    "classes = set()\n",
    "\n",
    "for i, c in enumerate(communities):\n",
    "    colors[list(c)] = i\n",
    "    classes.add(i)\n",
    "    \n",
    "num_classes = len(classes)\n",
    "labels = np.eye(len(classes))[colors.astype(int)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"500px\"\n",
       "            src=\"colored_graph.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f86f021a7c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def random_color():\n",
    "    return '#%02X%02X%02X' % (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
    "\n",
    "# uncomment for random colors\n",
    "# color_map = {cls: random_color() for cls in classes}\n",
    "color_map = {0: '#46FB47', 1: '#B9E6B5', 2: '#9F9EBF'}\n",
    "\n",
    "colored_graph = Network(width='100%', notebook=True)\n",
    "\n",
    "for node in G.nodes():\n",
    "    colored_graph.add_node(node, color=color_map[int(colors[node])])\n",
    "    \n",
    "for edge in G.edges():\n",
    "    colored_graph.add_edge(int(edge[0]), int(edge[1]))\n",
    "    \n",
    "colored_graph.show('colored_graph.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def renormalization(G):\n",
    "    A = nx.to_numpy_matrix(G)\n",
    "    I = np.eye(len(A))\n",
    "    A_tilde = A + I\n",
    "    D_tilde = np.zeros(A.shape, int)\n",
    "    np.fill_diagonal(D_tilde, np.sum(A_tilde, axis=1).flatten())\n",
    "    D_tilde = np.linalg.inv(D_tilde)\n",
    "    D_tilde = np.power(D_tilde, 0.5)\n",
    "    return D_tilde @ A_tilde @ D_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 1., 1., ..., 1., 0., 0.],\n",
       "        [1., 0., 1., ..., 0., 0., 0.],\n",
       "        [1., 1., 0., ..., 0., 1., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 1., 1.],\n",
       "        [0., 0., 1., ..., 1., 0., 1.],\n",
       "        [0., 0., 0., ..., 1., 1., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.to_numpy_matrix(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05882353 0.0766965  0.07312724 ... 0.09166985 0.         0.        ]\n",
      " [0.0766965  0.1        0.09534626 ... 0.         0.         0.        ]\n",
      " [0.07312724 0.09534626 0.09090909 ... 0.         0.0836242  0.        ]\n",
      " ...\n",
      " [0.09166985 0.         0.         ... 0.14285714 0.10482848 0.08908708]\n",
      " [0.         0.         0.0836242  ... 0.10482848 0.07692308 0.06537205]\n",
      " [0.         0.         0.         ... 0.08908708 0.06537205 0.05555556]]\n"
     ]
    }
   ],
   "source": [
    "A_hat = renormalization(G)\n",
    "print(A_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "def glorot_init(in_dim, out_dim):\n",
    "    sd = np.sqrt(6.0 / (in_dim + out_dim))\n",
    "    return np.random.uniform(-sd, sd, size=(in_dim, out_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "For each iteration of gradient descent, compute local gradients with **backpropagation** - compute global gradient as a chain of local gradients using chain rule of calculus.\n",
    "\n",
    "Starting from the last layer $l$:\n",
    "\n",
    "$$\n",
    "\\delta^{(l)}=\\frac{\\partial}{\\partial z^{(l)}}\\mathcal{L}\n",
    "$$\n",
    "\n",
    "Think of the **error signal** $\\delta^{(l)}$ as serving as the same function as $x$ during the forward pass.\n",
    "\n",
    "For layer $i$ in $l-1$ to $2$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\nabla_{w^{(i)}}\\mathcal{L}&=\\delta^{(i+1)}a^{(i-1)^{\\top}}&&\\quad a^{(i-1)}\\text{ is the incoming activation to layer } i \\text{ from layer }i-1\\\\\\\\\n",
    "    \\nabla_{b^{(i)}}\\mathcal{L}&=\\delta^{(i+1)}&&\\quad\\text{shift by error signal}\\\\\\\\  \n",
    "    \\delta^{i} &= W^{i^{\\top}}\\delta^{i+1}\\odot\\frac{\\partial}{\\partial z}f(z^{(i-1)})&&\\quad W^{i^{\\top}} \\text{ propagates the error signal backwards as a linear combination scaled by the derivative}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Think about the convexity of the latent space as discretized across dimensions of $z\\in\\mathbb{R}^{N}$. Each row of a hidden layer corresponds to a neuron, which corresponds to a position in the activation vector for a given input vector. As $\\frac{\\partial}{\\partial z}f(z)\\rightarrow 0$, for the particular combination of activations given to this neuron from the previous layer, the derivative indicates how this specific neuron is changing - which measures the convexity of this particular dimension in latent space $\\mathbb{R}^{N}$.\n",
    "\n",
    "As $\\frac{\\partial}{\\partial z}f(z)\\rightarrow 0$, the neuron reacts less to the incoming linear combination, which means that for this combination of features, there is a local minima in this latent space. So as backpropagation progresses across training, gradients of neurons converge to local minima.\n",
    "\n",
    "For each layer $i$, apply gradient descent:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    W^{(i)} &= W^{(i)} - \\alpha\\mathbb{E}[\\nabla_{W^{(i)}}\\mathcal{L}]\\\\\n",
    "    b^{(i)} &= b^{(i)} - \\alpha\\mathbb{E}[\\nabla_{b^{(i)}}\\mathcal{L}]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\nabla_{W_{j}}]=\\frac{1}{b}\\sum^{b}_{j=1}\\nabla_{W_{j}}\\mathcal{L} \\text{ is the expected gradient of the batch of samples and same for bias}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent(object):\n",
    "    def __init__(self, parameters, learning_rate):\n",
    "        self.parameters = parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        \n",
    "    def zero_gradients(self):\n",
    "        for layer in self.parameters:\n",
    "            layer.W_grad = np.zeros(layer.W.shape)\n",
    "            layer.b_grad = np.zeros(layer.b.shape)\n",
    "    \n",
    "    \n",
    "    def step(self):\n",
    "        for layer in self.parameters:\n",
    "            layer.W -= self.learning_rate * layer.W_grad\n",
    "            layer.b -= self.learning_rate * layer.b_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Convolutional Layer\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(\\hat{A}XW^{1}+b^{1})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCLayer(object):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.W = glorot_init(output_dim, input_dim)\n",
    "        self.W_grad = np.zeros(self.W.shape)\n",
    "        self.b = np.ones((output_dim, 1))\n",
    "        self.b_grad = np.zeros(self.b.shape)\n",
    "        \n",
    "        \n",
    "    def relu(self, x):\n",
    "        return np.vectorize(lambda i : i if i > 0 else 0)(x)\n",
    "    \n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        return (x > 0) * 1\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    inputs:\n",
    "    G (nx.Graph)   Normalized Laplacian matrix for a static graph.\n",
    "                   Dimensions: N x N where N is the number of nodes.\n",
    "    x (np.ndarray) Embedding matrix\n",
    "                   Dimensions: N x F where F is the number of features.\n",
    "    '''\n",
    "    def __call__(self, G, x):\n",
    "        # (nodes x nodes), (nodes x features), so need to transpose\n",
    "        # before taking linear combination\n",
    "        self.i = x # (nxf)\n",
    "        self.X = (G @ x).T # (n,n) x (n,f) -> (n,f).T -> (f,n)\n",
    "        self.z = self.W @ self.X + self.b # (h,f) x (f,n) + (h,1) -> (h,n). Broadcast bias vector.\n",
    "        self.a = self.relu(self.z) # (h,n), where n is number of samples/nodes\n",
    "        \n",
    "        # print('GC Layer')\n",
    "        # print(f'x.shape: {x.shape}')\n",
    "        # print(f'X.shape: {self.X.shape}')\n",
    "        # print(f'W.shape: {self.W.shape}')\n",
    "        # print(f'b.shape: {self.b.shape}')\n",
    "        # print(f'z.shape: {self.z.shape}')\n",
    "        # print(f'a.shape: {self.a.shape}')\n",
    "        \n",
    "        # transpose so can multiply by adjacency matrix in next layer\n",
    "        return self.a.T # (n,h)\n",
    "    \n",
    "    \n",
    "    def backward(self, G, error):\n",
    "        samples = self.X.shape[0] # batch size\n",
    "        #should this be self.X or self.i?\n",
    "        self.W_grad += error @ self.X.T # (h,n) x (n,f) -> (h,f) which matches W.shape \n",
    "        self.b_grad += error # (h,n)\n",
    "        return self.W.T @ error * self.relu_derivative(self.z)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(object):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.W = glorot_init(output_dim, input_dim)\n",
    "        self.W_grad = np.zeros(self.W.shape)\n",
    "        self.b = np.ones((self.output_dim, 1))\n",
    "        self.b_grad = np.zeros(self.b.shape)\n",
    "        \n",
    "        \n",
    "    def relu(self, x):\n",
    "        return np.vectorize(lambda i : i if i > 0 else 0)(x)\n",
    "    \n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        return (x > 0) * 1\n",
    "    \n",
    "\n",
    "    '''\n",
    "    inputs:\n",
    "    x (np.ndarray) Inputs to this layer\n",
    "    \n",
    "    outputs:\n",
    "    a (np.ndarray) Output activations\n",
    "    '''\n",
    "    def __call__(self, x):    \n",
    "        self.x = x # (f,n)\n",
    "        self.z = self.W @ x + self.b # (h,f) x (f,n) + (h,1). Broadcast bias vector.\n",
    "        self.a = self.relu(self.z)   # (h,n)\n",
    "        \n",
    "        # print('Linear layer')\n",
    "        # print(f'x.shape: {x.shape}')\n",
    "        # print(f'W.shape: {self.W.shape}')\n",
    "        # print(f'b.shape: {self.b.shape}')\n",
    "        # print(f'z.shape: {self.z.shape}')\n",
    "        \n",
    "        return self.a.T # (h,n)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    inputs:\n",
    "    error (np.ndarray) Error signal of shape (W.out_dim, batch_size) from subsequent layer\n",
    "    \n",
    "    outputs:\n",
    "    \n",
    "    '''\n",
    "    def backward(self, error):\n",
    "        self.W_grad += error @ self.x.T # (h,n) x (n,f)\n",
    "        self.b_grad += error            # (h,n)\n",
    "        return self.W.T @ error * self.relu_derivative(self.z) # (f,h) x (h,n) * (h,n)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Convolutional Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(object):\n",
    "    def __init__(self, graph, num_classes):\n",
    "        self.G = graph\n",
    "        self.nodes = self.G.shape[0]\n",
    "        self.embedding = np.eye(self.nodes)\n",
    "        self.l0 = GCLayer(self.nodes, 16)\n",
    "        self.l1 = GCLayer(16, 16)\n",
    "        self.l2 = Linear(16, num_classes)\n",
    "        self.parameters = [self.l0, self.l1, self.l2]\n",
    "        \n",
    "    \n",
    "    def __call__(self, x):\n",
    "        print(f'backward:\\nx.shape: {x.shape}')\n",
    "        a0 = self.l0(self.G, x)\n",
    "        # print(f'a0.shape: {a0.shape}')\n",
    "        a1 = self.l1(self.G, a0).T\n",
    "        a2 = self.l2(a1)\n",
    "        return softmax(a2)\n",
    "    \n",
    "    \n",
    "    def backward(self, x):\n",
    "        d2 = self.l2.backward(x)\n",
    "        print(f'd2.shape: {d2.shape}')\n",
    "        d1 = self.l1.backward(d2)\n",
    "        print(f'd1.shape: {d1.shape}')\n",
    "        d0 = self.l0.backwards(d1)\n",
    "        print(f'd0.shape: {d0.shape}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_ent(pred, labels):\n",
    "    # print(f'pred: {pred.shape}')\n",
    "    # print(f'labels: {labels.shape}')\n",
    "    \n",
    "    return -np.log(pred)[np.arange(pred.shape[0]), np.argmax(labels, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss, epochs, features, labels, opt):\n",
    "    for e in range(epochs):\n",
    "        output = model(features)\n",
    "        print(f'output: {output}')\n",
    "        loss = loss(output, labels)\n",
    "        \n",
    "        print(f'loss: {loss}\\n{loss.shape}')\n",
    "            \n",
    "        model.backward(loss)\n",
    "        opt.step()\n",
    "        opt.zero_gradients()\n",
    "        \n",
    "        print(f'epoch {e} loss: {loss.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l0: [[ 3.00718258e-02 -1.53550194e-01 -5.22957472e-02  2.38867911e-01\n",
      "  -3.43140842e-01 -2.62184604e-01  1.18298436e-01  2.25757411e-01\n",
      "  -2.51697058e-01  5.20261848e-02  2.71115803e-01 -2.01470680e-01\n",
      "  -2.18011005e-01 -2.71324449e-01 -1.94199274e-01  3.31600285e-01\n",
      "   2.15940420e-01 -2.27285934e-01  2.19086933e-01 -1.56526300e-01\n",
      "  -4.73167295e-02  3.04861602e-01  2.20073945e-01 -1.13544972e-01\n",
      "  -2.24882234e-01 -8.81045428e-02 -3.42469048e-01 -1.71524054e-01\n",
      "   2.04840995e-01 -3.35841207e-01  6.84807003e-02  7.19178943e-02\n",
      "  -2.73561708e-01 -8.17919806e-02]\n",
      " [-3.21138808e-01  2.70485065e-01  3.33191744e-01 -3.04881133e-01\n",
      "   2.70578168e-01  5.32789217e-02  1.67994857e-01  9.01940769e-02\n",
      "   5.67019342e-02 -3.32249515e-01 -2.00899480e-01  3.09585917e-02\n",
      "   1.86448460e-01 -1.72723412e-01 -1.48335817e-01  2.44146479e-01\n",
      "   3.29094152e-01  2.66634183e-01 -9.73358209e-02  6.84914868e-02\n",
      "  -1.00600551e-01 -1.10719467e-01 -2.23032033e-01 -1.81730783e-01\n",
      "  -3.15328660e-01  3.76300483e-03 -8.57348146e-02  6.42974679e-02\n",
      "   9.00263722e-02 -2.47613766e-01  3.00574069e-01  3.09261053e-01\n",
      "   7.08732035e-02 -7.77578019e-02]\n",
      " [-9.47861312e-02 -2.04835601e-01 -1.54661702e-01 -1.75605093e-01\n",
      "  -2.26131010e-01  3.23276679e-01  3.16627617e-01  6.78781596e-02\n",
      "   1.60249862e-01 -1.10584361e-01 -2.82632169e-01 -2.52893143e-02\n",
      "   6.02677002e-03 -2.85123156e-01  1.94233725e-02  3.40977090e-01\n",
      "  -7.27212397e-02 -1.13902126e-01  2.11622340e-01  1.76218153e-01\n",
      "  -1.29511368e-01  9.28633380e-02  2.79931109e-02 -1.40785419e-01\n",
      "  -2.69654052e-01 -1.29806609e-01 -2.98057330e-02  1.10116911e-01\n",
      "  -1.70255386e-01  9.77578196e-02 -2.07760459e-01  1.09205669e-01\n",
      "   1.92804424e-01  1.93711453e-01]\n",
      " [ 7.64375867e-02 -1.32328440e-01  1.36994762e-01  2.49150864e-01\n",
      "   8.68268463e-02  3.34221948e-01  3.30128972e-01 -2.30921080e-01\n",
      "  -3.30351878e-01 -2.35043071e-01  2.93407207e-01  3.14228554e-01\n",
      "  -2.00240025e-01 -9.66309408e-02  3.42081847e-02 -1.58080225e-01\n",
      "  -2.72959976e-02  1.35904719e-01  2.46572449e-04  1.49698373e-01\n",
      "   1.79828001e-02 -3.45440890e-01 -7.29537814e-02 -5.42688244e-03\n",
      "  -6.72864802e-02 -1.00945099e-01  4.25612995e-04 -3.79827457e-02\n",
      "  -2.83756488e-01 -1.56880211e-01  3.07249946e-01 -3.28019495e-01\n",
      "  -3.18698256e-01 -1.50244766e-01]\n",
      " [ 5.70497146e-02  3.40100510e-01  3.41312554e-01  3.41641737e-01\n",
      "  -2.70166441e-01  1.13956089e-01  1.66185664e-02 -2.26448385e-01\n",
      "   3.06891860e-01 -1.78844579e-01  3.45670416e-01  5.72919557e-02\n",
      "  -2.19430745e-01 -7.83957913e-02 -2.15000486e-01 -6.18198911e-02\n",
      "   6.55962759e-02  1.50055247e-01 -9.08184742e-03 -1.31920044e-01\n",
      "   5.36529569e-02 -4.03860073e-02 -9.72178623e-02 -1.23784869e-01\n",
      "  -2.02159954e-01 -3.37690158e-02 -5.65139755e-03  2.76488181e-01\n",
      "   1.58905589e-01  1.87123684e-01 -8.62982207e-02 -1.08260426e-01\n",
      "   1.07411542e-01  1.46211411e-01]\n",
      " [-2.67749022e-01 -2.54245182e-01 -3.04570343e-02 -2.35741655e-01\n",
      "   3.19834893e-01  2.33907049e-01  1.39677337e-02 -1.95186705e-01\n",
      "  -2.52935729e-01  3.31909672e-01  1.43443942e-01  2.49398382e-01\n",
      "  -7.81690964e-02 -1.72627255e-01 -1.38953417e-01  2.47264475e-01\n",
      "  -1.87172404e-02  1.13121656e-01  2.11814993e-01 -1.71140127e-01\n",
      "  -2.91280066e-01  1.61261278e-01  3.19665549e-01  3.14405143e-01\n",
      "  -6.58244994e-03  9.15853488e-02  1.61423685e-01  2.78797482e-01\n",
      "  -2.34002199e-01 -6.52073326e-02 -5.74412234e-02  1.35509439e-01\n",
      "  -5.20673609e-02  2.48108814e-01]\n",
      " [ 2.40361873e-01 -2.97774789e-01 -1.37349957e-01  3.32293034e-01\n",
      "  -3.21727054e-01 -5.27052876e-03  3.13415877e-01  2.15171812e-01\n",
      "  -1.42492050e-01  6.66725374e-02 -4.76813826e-02  6.40148679e-02\n",
      "   2.72799460e-01  3.74269781e-02 -4.94222869e-03 -1.25213100e-01\n",
      "  -1.63944995e-01  2.92928683e-02 -2.89415627e-01  9.39718691e-02\n",
      "   2.05355564e-01  3.15058314e-01  1.27911448e-01 -8.11073196e-03\n",
      "  -1.01052623e-02  3.23334340e-01 -1.99984050e-01 -6.12119654e-02\n",
      "   3.39250263e-01 -3.26725850e-01  1.39483101e-01 -3.28970790e-01\n",
      "  -1.24096780e-01 -2.95469119e-01]\n",
      " [-3.04228098e-01 -2.69225601e-01 -2.29137222e-01  8.84636494e-02\n",
      "  -4.26825161e-02  2.29256853e-01 -1.80277259e-01 -2.14737783e-01\n",
      "   1.46808390e-01  2.48234006e-01  4.09151177e-02  1.41626613e-01\n",
      "   7.28237544e-02  4.10269373e-02  2.49688420e-01  2.90815044e-01\n",
      "   2.42215060e-01 -1.70110574e-01  2.61578153e-01 -4.49431227e-02\n",
      "   1.58998346e-01 -6.05242517e-02 -2.14195071e-01  1.42734510e-01\n",
      "  -1.79694853e-01  2.43404703e-01  2.24544653e-01  1.74672381e-02\n",
      "  -7.87454076e-02  6.29640588e-02 -2.51131006e-01  2.13576004e-01\n",
      "   3.22733592e-01  1.93848219e-01]\n",
      " [-1.80593953e-01  2.54445478e-01  2.13468343e-01 -3.02290584e-01\n",
      "  -1.86210493e-01  6.21359015e-02 -2.51156410e-01  1.23627346e-01\n",
      "   3.40999713e-01 -1.48435379e-01  1.80765662e-01 -3.14175194e-01\n",
      "  -1.16022527e-01  3.07995208e-01  9.45817806e-02  7.05627842e-02\n",
      "   2.96655049e-01 -2.20538973e-01 -3.34061897e-01 -2.14724295e-01\n",
      "   1.51532260e-02 -2.89461308e-03  2.08186414e-01  2.49024781e-01\n",
      "  -1.98869894e-01 -4.34614215e-02 -5.43049827e-02 -3.08500851e-01\n",
      "  -3.39527897e-01  2.00755552e-01 -1.55667568e-01  1.50854700e-01\n",
      "  -5.44862157e-02 -2.47104156e-01]\n",
      " [-2.13027229e-01 -1.28992590e-01  2.11428094e-01 -3.37662730e-01\n",
      "  -3.12388546e-01  4.57264080e-02  1.29426247e-01  1.57139205e-01\n",
      "  -1.40685749e-02 -9.16901128e-02  2.35538194e-01 -3.17564224e-02\n",
      "  -1.23761377e-01 -2.82171953e-01 -3.04537534e-01 -2.83397344e-01\n",
      "   1.26582746e-01  1.25217413e-01 -1.77934157e-01  9.73145417e-02\n",
      "  -2.98509130e-01  2.58366528e-01 -2.70472239e-01 -2.29284892e-01\n",
      "  -2.26011905e-02  1.91183227e-01  2.45566364e-01 -2.00650157e-01\n",
      "  -2.93311117e-01  2.00166043e-01  3.29089661e-02  1.98323187e-01\n",
      "   2.91017128e-01 -1.31824545e-02]\n",
      " [-2.80220394e-02  6.85747705e-02  6.88100697e-02  3.03001590e-03\n",
      "  -1.33798479e-01  2.86501852e-02  2.94398022e-01  3.26007159e-01\n",
      "  -7.21956120e-02  2.06976796e-01  9.35918142e-02 -1.87082850e-01\n",
      "  -3.10932847e-01 -3.26689858e-01 -2.61298742e-01 -1.93842454e-01\n",
      "   2.27953651e-01 -1.48615621e-01  1.94726909e-01  3.23256979e-03\n",
      "  -2.50489933e-01  1.92629374e-01  2.91907229e-01  3.06932312e-01\n",
      "   1.41637275e-01  1.34349254e-01  3.22520424e-02 -9.06089611e-02\n",
      "   3.34263341e-01 -3.00954757e-01  2.75519616e-01 -1.63553408e-01\n",
      "   5.15983769e-02  8.91401202e-03]\n",
      " [ 3.77426398e-02  1.01960518e-01 -2.17909895e-01 -1.57978134e-01\n",
      "  -2.43568843e-01 -3.25414816e-01  3.04325196e-01 -1.06171853e-01\n",
      "  -2.70501577e-01 -8.42975302e-02 -8.03121066e-02  1.14608893e-01\n",
      "  -1.77021877e-01  1.11877300e-01 -2.78172290e-01  5.60233550e-02\n",
      "  -2.72371568e-01  3.34317034e-02  1.36843845e-02 -1.41173144e-01\n",
      "  -3.06718132e-02 -3.19621211e-01  6.85913077e-02 -3.46072925e-01\n",
      "   1.25649456e-03  1.19589219e-03 -3.04376963e-01  3.11656173e-01\n",
      "   7.52811948e-02  1.19166955e-01 -2.57912593e-02  1.41524552e-01\n",
      "  -2.20963166e-01  1.02247932e-01]\n",
      " [ 4.71871157e-02  3.14636254e-01  2.05553027e-01  5.91047738e-02\n",
      "  -3.09309861e-02  1.65204066e-01  2.16323639e-01  2.96036009e-01\n",
      "   2.26119653e-01 -3.25655622e-01  1.89003339e-01  1.50878209e-02\n",
      "   2.67004176e-01 -2.87093192e-02  2.82134284e-02  2.82507878e-01\n",
      "   3.90127403e-02 -2.16773279e-01  1.25477636e-01 -3.36816896e-01\n",
      "  -8.30749363e-02  2.13862888e-01  5.27412328e-02 -2.41250772e-01\n",
      "  -1.17880080e-01  3.12997617e-01 -3.25359008e-01  1.92724273e-01\n",
      "  -2.20743737e-01 -1.98384341e-01  1.05721764e-01 -2.46580333e-01\n",
      "  -8.24405812e-02 -6.53090316e-02]\n",
      " [ 1.10134484e-01  8.19098862e-03 -1.20653940e-02  1.66427900e-02\n",
      "   2.12201078e-01  2.10888327e-01 -2.44080298e-01  3.41874667e-01\n",
      "  -2.99459297e-01  3.43511268e-01 -2.60881706e-01 -3.03504355e-01\n",
      "   1.41359901e-01  3.19376072e-01  1.46982488e-01  1.12986054e-01\n",
      "  -2.53797431e-01 -2.21463363e-01 -2.37834223e-01 -2.83728295e-01\n",
      "   3.26918457e-01  6.83199147e-03 -3.40330827e-02 -1.17986623e-01\n",
      "  -4.49498912e-03 -2.46920481e-01 -2.41849517e-01  2.18472976e-01\n",
      "  -2.00029779e-01 -2.86451882e-01 -7.81103524e-02  2.72557880e-01\n",
      "  -4.94373734e-02  1.84500832e-01]\n",
      " [ 3.43129936e-01 -3.32510211e-01 -2.71448230e-01  1.34921681e-01\n",
      "   2.70659994e-01 -5.21255579e-03 -2.97727070e-01 -2.85138958e-01\n",
      "  -2.72826991e-01 -2.06973920e-01  2.61445452e-01  3.07073557e-01\n",
      "  -2.52438689e-01  2.48533337e-01  1.78164484e-01  1.28983478e-01\n",
      "   2.46916584e-01 -1.86453362e-01  1.33977450e-02 -1.08541773e-01\n",
      "   1.10001112e-01 -1.50487326e-01  1.70879543e-03 -1.36293332e-01\n",
      "   3.63934521e-02 -2.34778034e-01  4.74921158e-02 -9.90049379e-03\n",
      "  -2.58062030e-01  3.02708228e-02 -2.07506193e-01  1.17890902e-01\n",
      "   4.02613250e-02 -1.85413741e-01]\n",
      " [ 1.08000196e-02 -1.28177321e-01  2.40126362e-01 -3.64587825e-02\n",
      "  -2.77069084e-01  2.78231158e-01  2.46705175e-01 -1.48035006e-01\n",
      "  -1.66959465e-01 -3.00426725e-01 -1.26348484e-01 -3.10492281e-01\n",
      "   3.07742694e-01  1.50492561e-01  3.71761844e-02 -9.71716738e-02\n",
      "  -2.36125428e-01 -4.64480316e-02 -1.52862368e-01  3.19416241e-01\n",
      "  -2.78422210e-01 -6.44353714e-02 -3.40606789e-01  4.71526096e-02\n",
      "   5.30768045e-02 -2.51393830e-01  1.19316523e-01 -2.47424340e-01\n",
      "   6.39576891e-03 -9.09243864e-02 -1.73823500e-01 -2.51991138e-01\n",
      "  -2.63762841e-01 -3.10114603e-01]]\t[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "l1: [[-0.05637885  0.23443774  0.3039006   0.10503387 -0.10402031  0.15580753\n",
      "  -0.1612838   0.19627425  0.35895297 -0.3508308   0.14287423 -0.1239518\n",
      "   0.2271506   0.38538623 -0.23924627 -0.38342996]\n",
      " [ 0.12367003 -0.35612947  0.14543893  0.09285644 -0.41603876 -0.09197766\n",
      "  -0.0350677  -0.41470838 -0.10938689  0.05678281 -0.42532657 -0.2919845\n",
      "   0.3447922  -0.12519842 -0.16475829  0.07114977]\n",
      " [ 0.01919513  0.33121641 -0.19715548 -0.17336092 -0.16594198 -0.34554044\n",
      "  -0.17751642  0.19823736 -0.1369369  -0.18527041  0.39520862  0.43256163\n",
      "  -0.14085737 -0.02154536  0.28548498 -0.34692532]\n",
      " [ 0.00702749  0.28911095  0.23393337  0.27366433 -0.38836911  0.15703101\n",
      "   0.40122406  0.1740256  -0.07807525 -0.25371029  0.39492721 -0.08139671\n",
      "   0.1878237   0.14924338  0.29629942  0.38169238]\n",
      " [-0.02615301 -0.33972746  0.05775125  0.4154991  -0.05723155  0.2754259\n",
      "  -0.05434344  0.26893059 -0.23416594 -0.07709519  0.20000955  0.11891776\n",
      "   0.2330844   0.39510161 -0.10738138  0.34268748]\n",
      " [-0.2524633  -0.37828298 -0.22566745  0.24265967 -0.10732182  0.07056698\n",
      "  -0.22369251 -0.05321329 -0.27673645 -0.1098809   0.31517556  0.30829957\n",
      "   0.38112007 -0.08163156  0.4176929  -0.01506543]\n",
      " [ 0.32659618 -0.07891291 -0.15608096  0.12338143  0.18131579  0.2098785\n",
      "   0.09051232  0.31041949  0.12982313 -0.19876505  0.31044834 -0.27641369\n",
      "   0.03265359 -0.09792156  0.10356383  0.04207698]\n",
      " [-0.0312687   0.28270952 -0.0857384   0.10349288  0.00061411  0.00077033\n",
      "   0.09431285  0.42899888 -0.21828212  0.20648939  0.37242896  0.073103\n",
      "   0.3296501   0.16632808 -0.12867554  0.27773387]\n",
      " [-0.2276911   0.36519743  0.36273379  0.11217689  0.01058553 -0.17333367\n",
      "  -0.04238147  0.18805186 -0.18211582 -0.25214836 -0.15888369  0.06342304\n",
      "  -0.03245324  0.00226774 -0.01104915  0.24181791]\n",
      " [-0.09514511  0.23255829  0.3130407  -0.30012072  0.09989993  0.01145886\n",
      "   0.42371647 -0.3612839   0.18313833 -0.19645404  0.42863185 -0.12580932\n",
      "  -0.35626084  0.4185071  -0.01797611 -0.01544381]\n",
      " [ 0.022989    0.38371362 -0.0080507  -0.40691901  0.37712838  0.21415076\n",
      "   0.19954221 -0.34183655  0.23001857  0.18002853  0.36592877  0.00785603\n",
      "   0.18369262  0.32133727 -0.06004863 -0.01114688]\n",
      " [-0.32715686 -0.26845395  0.08637783 -0.1748699  -0.27480426  0.00262206\n",
      "   0.39029135 -0.23159243  0.20705112  0.22661401  0.23041167  0.36762415\n",
      "  -0.15345214  0.15496908 -0.28421223  0.18149636]\n",
      " [-0.00924622 -0.05029766  0.12693836 -0.01867707  0.24333891  0.05523896\n",
      "   0.30238175 -0.14507009  0.16249957 -0.16426263  0.13925295 -0.12602863\n",
      "  -0.37991453 -0.04857668  0.18041548 -0.2100816 ]\n",
      " [ 0.10250539 -0.32768212  0.11892634 -0.38253057 -0.39176902  0.01034429\n",
      "  -0.17379832 -0.13299021  0.12922609  0.38874865  0.01406588  0.18493404\n",
      "   0.07461783 -0.09098823  0.14879748 -0.22761696]\n",
      " [ 0.30155797 -0.17972351 -0.32852126 -0.05959624 -0.19749574 -0.19595879\n",
      "   0.12315104 -0.0433772   0.35375819  0.23850094 -0.13933264  0.17644458\n",
      "  -0.2259299  -0.255338    0.15932148 -0.072144  ]\n",
      " [-0.26395463 -0.09497737 -0.34768891 -0.15368108  0.33729796  0.38301871\n",
      "   0.13637368 -0.26359324  0.02223846 -0.16384412  0.04793312  0.03076972\n",
      "  -0.03021309  0.23197754  0.33510591  0.28562329]]\t[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "l2: [[ 0.5156951   0.46199605 -0.42744864 -0.43329989  0.55854045 -0.51698601\n",
      "   0.40411476 -0.038769   -0.23714541  0.26216606 -0.03125299 -0.15056503\n",
      "  -0.47907141  0.36100359 -0.01346783  0.28891282]\n",
      " [-0.14489574 -0.25905421  0.26366161  0.3895049   0.30700501 -0.45263718\n",
      "  -0.21029932 -0.50092651  0.55792551 -0.36223535 -0.14472386 -0.15854608\n",
      "  -0.29313532 -0.34377444  0.24968277  0.55784909]\n",
      " [ 0.17744299 -0.34881378  0.33273149  0.14964637 -0.49454579 -0.05480492\n",
      "  -0.11446325 -0.04777969 -0.36718426  0.06199474  0.19733005  0.36687322\n",
      "   0.28544365 -0.5191687   0.33167911  0.17300442]]\t[[1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "backward:\n",
      "x.shape: (34, 34)\n",
      "output: [[0.05044098 0.02769159 0.02921804]\n",
      " [0.03647556 0.02806421 0.03295791]\n",
      " [0.03853068 0.0278217  0.0320918 ]\n",
      " [0.03006913 0.03056376 0.03119877]\n",
      " [0.02629644 0.03192024 0.02785779]\n",
      " [0.02937716 0.03081785 0.0277485 ]\n",
      " [0.02847549 0.03050737 0.02810698]\n",
      " [0.02595336 0.02952804 0.03119307]\n",
      " [0.02694755 0.02861868 0.03002703]\n",
      " [0.02609029 0.02876627 0.02974203]\n",
      " [0.0264782  0.03121589 0.027871  ]\n",
      " [0.02119321 0.03314513 0.02785274]\n",
      " [0.02430004 0.03267063 0.0290287 ]\n",
      " [0.02623008 0.03026883 0.0307176 ]\n",
      " [0.02481687 0.03073439 0.02792538]\n",
      " [0.02537606 0.03021162 0.02777051]\n",
      " [0.02656157 0.03030907 0.02793247]\n",
      " [0.02293712 0.03087428 0.03048808]\n",
      " [0.02465461 0.0295915  0.0271648 ]\n",
      " [0.0234088  0.03066928 0.02953192]\n",
      " [0.02457954 0.03025808 0.02900834]\n",
      " [0.02450302 0.02878107 0.03047971]\n",
      " [0.02522439 0.02850013 0.02941393]\n",
      " [0.03075904 0.02822047 0.02911519]\n",
      " [0.02823907 0.02801072 0.03029433]\n",
      " [0.02781303 0.0276038  0.03011859]\n",
      " [0.02587689 0.03002295 0.02758597]\n",
      " [0.02817589 0.02846209 0.02961416]\n",
      " [0.02481044 0.02896081 0.03102551]\n",
      " [0.02897967 0.02906495 0.02784244]\n",
      " [0.02624373 0.02858579 0.03010591]\n",
      " [0.02998962 0.02816167 0.03071999]\n",
      " [0.04714673 0.02620839 0.02954386]\n",
      " [0.06304575 0.02516874 0.02870694]]\n",
      "loss: [[3.53296892 3.57326022 3.58193888 3.4879402  3.58064278 3.58457351\n",
      "  3.57173751 3.52241492 3.61386276 3.54855171 3.58016861 3.58082394\n",
      "  3.42127887 3.49763666 3.69623157 3.67394926 3.57796541 3.47783179\n",
      "  3.70279147 3.52228357 3.70584072 3.54803756 3.67994402 3.48157141\n",
      "  3.56704887 3.58225056 3.65440506 3.56928855 3.69649068 3.54116065\n",
      "  3.64032811 3.5069041  3.05449067 2.76389459]]\n",
      "(1, 34)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (3,1) doesn't match the broadcast shape (3,34)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-62d6ee4b54ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGCN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradientDescent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_ent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-cb6e50675147>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loss, epochs, features, labels, opt)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'loss: {loss}\\n{loss.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-1e792c819995>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0md2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'd2.shape: {d2.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0md1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-12cc7b3d0cc2>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, error)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_grad\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0merror\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;31m# (h,n) x (n,f)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_grad\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0merror\u001b[0m            \u001b[0;31m# (h,n)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0merror\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (f,h) x (h,n) * (h,n)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: non-broadcastable output operand with shape (3,1) doesn't match the broadcast shape (3,34)"
     ]
    }
   ],
   "source": [
    "features = np.eye(G.number_of_nodes())\n",
    "model = GCN(A_hat, num_classes)\n",
    "opt = GradientDescent(model.parameters, lr)\n",
    "train(model, cross_ent, epochs, features, labels, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
