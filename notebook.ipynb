{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e497f496",
   "metadata": {},
   "source": [
    "### TODO\n",
    "1. [GCN Backprop](https://github.com/dmlc/dgl/issues/4021)\n",
    "2. Visualize loss\n",
    "3. [Embeddings](https://beta.openai.com/docs/guides/embeddings/what-are-embeddings)\n",
    "4. Node embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8473ca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import config\n",
    "import argparse\n",
    "import random\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from pyvis.network import Network\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.algorithms.community.modularity_max import greedy_modularity_communities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f085c8e4",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64a40ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "epochs = 20000\n",
    "lr = 0.001\n",
    "hidden_dim = 16\n",
    "hidden_dim2 = 20\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6938d9fe",
   "metadata": {},
   "source": [
    "### Graph data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4865222a",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.karate_club_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74bd7c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes, num_edges = G.number_of_nodes(), G.number_of_edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d5c581",
   "metadata": {},
   "source": [
    "### Generate labels from communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3c24258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes: 3\t samples: 34\n"
     ]
    }
   ],
   "source": [
    "communities = greedy_modularity_communities(G)\n",
    "colors = np.zeros(G.number_of_nodes())\n",
    "classes = set()\n",
    "\n",
    "for i, c in enumerate(communities):\n",
    "    colors[list(c)] = i\n",
    "    classes.add(i)\n",
    "    \n",
    "num_classes = len(classes)\n",
    "labels = (np.eye(len(classes))[colors.astype(int)]).T\n",
    "\n",
    "classes, samples = labels.shape\n",
    "print(f'classes: {classes}\\t samples: {samples}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0e1534",
   "metadata": {},
   "source": [
    "### Color nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2babfe8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local cdn resources have problems on chrome/safari when used in jupyter-notebook. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"600px\"\n",
       "            src=\"colored_graph.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x128eeba50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def random_color():\n",
    "    return '#%02X%02X%02X' % (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
    "\n",
    "# uncomment for random colors\n",
    "# color_map = {cls: random_color() for cls in classes}\n",
    "color_map = {0: '#46FB47', 1: '#B9E6B5', 2: '#9F9EBF'}\n",
    "\n",
    "colored_graph = Network(width='100%', notebook=True)\n",
    "\n",
    "for node in G.nodes():\n",
    "    colored_graph.add_node(node, color=color_map[int(colors[node])])\n",
    "    \n",
    "for edge in G.edges():\n",
    "    colored_graph.add_edge(int(edge[0]), int(edge[1]))\n",
    "    \n",
    "colored_graph.show('colored_graph.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a810100",
   "metadata": {},
   "source": [
    "#### Renormalization trick\n",
    "\n",
    "$A$ is the adjacency matrix, $I$ is the identity matrix, and $N$ is the cardinality of the set of nodes in the graph.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\tilde{A} &= A + I_{N}\\\\\n",
    "       \\tilde{\\mathcal{D}}_{ii} &= \\sum_{i}\\tilde{A}_{ij}\\\\\n",
    "    \\hat{\\mathcal{A}}&=\\tilde{\\mathcal{D}}^{-\\frac{1}{2}}\\tilde{\\mathcal{A}}\\tilde{\\mathcal{D}}^{-\\frac{1}{2}}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08e7262",
   "metadata": {},
   "source": [
    "In this notebook, each column of the design matrix `A_hat` corresponds to a single node and each row is a feature dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1364cf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def renormalization(G):\n",
    "    A = np.asarray(nx.to_numpy_matrix(G))\n",
    "    I = np.eye(len(A))\n",
    "    A_tilde = A + I\n",
    "    D_tilde = np.zeros(A.shape, int)\n",
    "    np.fill_diagonal(D_tilde, np.sum(A_tilde, axis=1).flatten())\n",
    "    D_tilde = np.linalg.inv(D_tilde)\n",
    "    D_tilde = np.power(D_tilde, 0.5)\n",
    "    return D_tilde @ A_tilde @ D_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77a25749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 4., 5., ..., 2., 0., 0.],\n",
       "        [4., 0., 6., ..., 0., 0., 0.],\n",
       "        [5., 6., 0., ..., 0., 2., 0.],\n",
       "        ...,\n",
       "        [2., 0., 0., ..., 0., 4., 4.],\n",
       "        [0., 0., 2., ..., 4., 0., 5.],\n",
       "        [0., 0., 0., ..., 4., 5., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.to_numpy_matrix(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7151e496",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02325581 0.11136921 0.13076645 ... 0.06502561 0.         0.        ]\n",
      " [0.11136921 0.03333333 0.18786729 ... 0.         0.         0.        ]\n",
      " [0.13076645 0.18786729 0.02941176 ... 0.         0.0549235  0.        ]\n",
      " ...\n",
      " [0.06502561 0.         0.         ... 0.04545455 0.13655775 0.12182898]\n",
      " [0.         0.         0.0549235  ... 0.13655775 0.02564103 0.11437725]\n",
      " [0.         0.         0.         ... 0.12182898 0.11437725 0.02040816]]\n",
      "(34, 34)\n"
     ]
    }
   ],
   "source": [
    "# Must pre-process offline\n",
    "A_hat = renormalization(G)\n",
    "print(A_hat)\n",
    "print(A_hat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e354f0b",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "71522a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_nodes (20): [0, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 26, 28, 31, 32]\n",
      "test_nodes  (14): [1, 2, 3, 4, 5, 6, 7, 11, 24, 25, 27, 29, 30, 33]\n"
     ]
    }
   ],
   "source": [
    "train_split = .6\n",
    "test_split = 1 - train_split\n",
    "train_nodes = random.sample(list(range(34)), int(train_split * num_nodes))\n",
    "test_nodes = list(set(range(34)) - set(train_nodes))\n",
    "\n",
    "train_nodes.sort()\n",
    "test_nodes.sort()\n",
    "print(f'train_nodes ({len(train_nodes)}): {train_nodes}')\n",
    "print(f'test_nodes  ({len(test_nodes)}): {test_nodes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c169f0",
   "metadata": {},
   "source": [
    "### Train-Test Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fdef24d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = np.zeros((num_nodes, num_nodes))\n",
    "train_mask[train_nodes, :] = np.ones(num_nodes)\n",
    "train_mask.T[train_nodes, :] = np.ones(num_nodes)\n",
    "test_mask = np.logical_not(train_mask).astype(int)\n",
    "\n",
    "assert (train_mask + test_mask == np.ones((num_nodes, num_nodes))).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3245bd",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1d53e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(model, scheme):\n",
    "    for i, layer in enumerate(model.parameters):\n",
    "        model.parameters[i].W = scheme(*layer.W.shape)\n",
    "        model.parameters[i].b = scheme(*layer.b.shape)\n",
    "        \n",
    "    return model\n",
    "\n",
    "def glorot_init(in_dim, out_dim):\n",
    "    sd = np.sqrt(6.0 / (in_dim + out_dim))\n",
    "    return np.random.uniform(-sd, sd, size=(in_dim, out_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d36df3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent(object):\n",
    "    def __init__(self, parameters, learning_rate):\n",
    "        self.parameters = parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        \n",
    "    def zero_gradients(self):\n",
    "        for layer in self.parameters:\n",
    "            layer.dW = np.zeros(layer.W.shape)\n",
    "            layer.db = np.zeros(layer.b.shape)\n",
    "    \n",
    "    \n",
    "    def step(self):\n",
    "        for i, layer in enumerate(self.parameters):\n",
    "            # TODO: Replace with assertion\n",
    "            if np.any(np.isnan(layer.db)):\n",
    "                print(f'nans layer {i}')\n",
    "                \n",
    "            layer.W -= self.learning_rate * layer.dW\n",
    "            layer.b -= self.learning_rate * layer.db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31551367",
   "metadata": {},
   "source": [
    "### Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad2e8cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_(x):\n",
    "    return (x > 0).astype(int)\n",
    "\n",
    "def softmax(x, axis=0):\n",
    "    return np.exp(x)/np.sum(np.exp(x), axis=axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1105b6a",
   "metadata": {},
   "source": [
    "### Graph Convolutional Layer\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(\\hat{A}XW^{1}+b^{1})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c38d5bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCLayer(object):\n",
    "    def __init__(self, input_dim, output_dim, name=''):\n",
    "        self.name = name\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.W = glorot_init(output_dim, input_dim)\n",
    "        self.dW = np.zeros(self.W.shape)\n",
    "        self.b = np.ones((output_dim, 1))\n",
    "        self.db = np.zeros(self.b.shape)\n",
    "        self.H = None\n",
    "            \n",
    "    '''\n",
    "    inputs:\n",
    "    G (nx.Graph)   Normalized Laplacian matrix for a static graph.\n",
    "                   Dimensions: N x N where N is the number of nodes.\n",
    "    x (np.ndarray) Embedding matrix\n",
    "                   Dimensions: N x F where F is the number of features.\n",
    "    '''\n",
    "    def __call__(self, G, x, activation=None):\n",
    "        if not activation:\n",
    "            activation = lambda x: x\n",
    "            \n",
    "        # (nodes x nodes), (nodes x features), so need to transpose\n",
    "        # before taking linear combination\n",
    "        self.z = x # (n,f)\n",
    "        \n",
    "        # need to apply the activations along feature/hidden dimension\n",
    "        # since x is (n,f), transpose to apply activations, then transpose back\n",
    "        # to dot with the adjacency matrix\n",
    "        self.a = activation(x.T).T\n",
    "        \n",
    "        # (n,n) x (n,f) -> (n,f).T -> (f,n) so can left-multiply weight with features\n",
    "        # this is purely stylistic preference.\n",
    "        X = (G @ self.a).T\n",
    "        \n",
    "        #print(f'({self.name}) W.shape: {self.W.shape}\\t X.shape: {X.shape}')\n",
    "        \n",
    "        # transpose so can multiply by adjacency matrix in next layer, (n,h)\n",
    "        self.H = (self.W @ X + self.b).T # (h,f) x (f,n) + (h,1) -> (h,n). Broadcast bias vector.\n",
    "        return self.H\n",
    "    \n",
    "    \n",
    "    def backward(self, G, error, derivative=None):\n",
    "        if not derivative:\n",
    "            derivative = lambda x: x\n",
    "            \n",
    "        # (l1) W.T.shape: (16, 20)\t error.shape: (20, 34)\t a.shape: (34, 16)\t z.shape: (34, 16)\n",
    "        #print(f'({self.name}) W.T.shape: {self.W.T.shape}\\t error.shape: {error.shape}\\t a.shape: {self.a.shape}\\t z.shape: {self.z.shape}')\n",
    "        #sys.exit(0)\n",
    "        #self.dW = error @ self.a.T # (h,n) @ (n,f) -> (h,f) which matches W.shape\n",
    "        self.dW = error @ self.a\n",
    "        self.db = np.sum(error, axis=1, keepdims=True) # (h,n)\n",
    "        \n",
    "        # (h,f) @ (f,n) * ((n,n) @ (n,f)).T\n",
    "        return self.W.T @ error * (G @ derivative(self.z)).T\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5913b5c",
   "metadata": {},
   "source": [
    "### Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c227db77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(object):\n",
    "    def __init__(self, input_dim, output_dim, name=''):\n",
    "        self.name = name\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.W = glorot_init(output_dim, input_dim)\n",
    "        self.dW = np.zeros(self.W.shape)\n",
    "        self.b = np.ones((self.output_dim, 1))\n",
    "        self.db = np.zeros(self.b.shape)    \n",
    "\n",
    "    '''\n",
    "    inputs:\n",
    "    x (np.ndarray) Inputs to this layer\n",
    "    \n",
    "    outputs:\n",
    "    a (np.ndarray) Output activations\n",
    "    '''\n",
    "    def __call__(self, x, activation=None):\n",
    "        if not activation:\n",
    "            activation = lambda x: x\n",
    "            \n",
    "        self.z = x # (f,n)\n",
    "        self.a = activation(x)\n",
    "        #print(f'({self.name}) W.shape: {self.W.shape}\\t a.shape: {self.a.shape}')\n",
    "        return self.W @ self.a + self.b # (h,f) x (f,n) + (h,1) -> (h,n). Broadcast bias vector.\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    inputs:\n",
    "    error (np.ndarray) Error signal of shape (W.out_dim, batch_size) from subsequent layer\n",
    "    \n",
    "    outputs:\n",
    "    \n",
    "    '''\n",
    "    def backward(self, error, derivative=None):\n",
    "        if not derivative:\n",
    "            derivative = lambda x: x\n",
    "            \n",
    "        batch_size = error.shape[1]\n",
    "        \n",
    "        #print(f'({self.name}) W.T.shape: {self.W.T.shape}\\t error.shape: {error.shape}\\t a.T.shape: {self.a.T.shape}\\t z.shape: {self.z.shape}')\n",
    "        \n",
    "        self.dW = error @ self.a.T # (h,n) x (n,f)        \n",
    "        self.db = np.sum(error, axis=1, keepdims=True) # (h,n)\n",
    "        \n",
    "        return self.W.T @ error * derivative(self.z) # (f,h) x (h,n) * (h,n)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995c89d9",
   "metadata": {},
   "source": [
    "### Graph Convolutional Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db447724",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(object):\n",
    "    def __init__(self, graph, num_classes):\n",
    "        self.G = graph\n",
    "        self.nodes = self.G.shape[0]\n",
    "        self.embedding = np.eye(self.nodes)\n",
    "        self.l0 = GCLayer(self.nodes, hidden_dim, name='l0')\n",
    "        self.l1 = GCLayer(hidden_dim, hidden_dim2, name='l1')\n",
    "        self.l2 = Linear(hidden_dim2, num_classes, name='l2')\n",
    "        self.parameters = [self.l0, self.l1, self.l2]\n",
    "        \n",
    "    \n",
    "    def __call__(self, x):\n",
    "        a0 = self.l0(self.G, x, activation=relu)\n",
    "        a1 = self.l1(self.G, a0, activation=relu).T # transpose b/c Linear layer expects (f,n)\n",
    "        a2 = self.l2(a1)\n",
    "        return softmax(a2)\n",
    "    \n",
    "    \n",
    "    def backward(self, x):\n",
    "        # Transpose errors from (34,3) -> (3,34) because linear weights are (16,34), but transposed for BP\n",
    "        # so computation must be (labels, batch_size) x (batch_size, hidden_dim) to get error @ x.T\n",
    "        d2 = self.l2.backward(x, derivative=relu_)\n",
    "        d1 = self.l1.backward(self.G, d2, derivative=relu_)\n",
    "        self.l0.backward(self.G, d1, derivative=None)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53471abd",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "\n",
    "#### Layer 1\n",
    "$$\n",
    "\\begin{align}\n",
    "    &\\text{ReLU}\n",
    "    \\Biggl(\n",
    "    \\\n",
    "        \\underset{\\mathcal{W}^{(1)}\\ \\in\\ \\mathbb{R}^{16\\times34}}{\n",
    "        \\begin{bmatrix}\n",
    "            w_{1,1} & \\ldots & w_{1,34}\\\\\n",
    "            \\vdots & \\ddots & \\vdots\\\\\n",
    "            w_{16,1} & \\ldots & w_{16,34}\n",
    "        \\end{bmatrix}}\n",
    "        \\Biggl(\n",
    "        \\\n",
    "            \\underset{\\hat{\\mathcal{A}}\\ \\in\\ \\mathbb{R}^{34\\times34}}{\n",
    "            \\begin{bmatrix}\n",
    "                \\alpha_{1,1} & \\ldots & \\alpha_{1,34}\\\\\n",
    "                \\vdots & \\ddots & \\vdots\\\\\n",
    "                \\alpha_{34,1} & \\ldots & \\alpha_{34,34}\n",
    "            \\end{bmatrix}}\n",
    "            \\ \n",
    "            \\underset{\\mathcal{X}\\ \\in\\ \\mathbb{R}^{34\\times34}}{\n",
    "            \\begin{bmatrix}\n",
    "                x_{1,1} & \\ldots & x_{1,34}\\\\\n",
    "                \\vdots & \\ddots & \\vdots\\\\\n",
    "                x_{34,1} & \\ldots & x_{34,34}\n",
    "            \\end{bmatrix}}\n",
    "        \\\n",
    "        \\Biggl)^{\\top}\n",
    "        +\n",
    "        \\underset{\\mathcal{b}^{(1)}\\ \\in\\ \\mathbb{R}^{16\\times34}}{\n",
    "        \\begin{bmatrix}\n",
    "            b_{1,1} & \\ldots & b_{1,34}\\\\\n",
    "            \\vdots & \\ddots & \\vdots\\\\\n",
    "            b_{16,1} & \\ldots & b_{16,34}\n",
    "        \\end{bmatrix}}\n",
    "    \\\n",
    "    \\Biggr)\\\\\n",
    "    =\\\n",
    "    &\\text{ReLU}\n",
    "    \\Biggl(\n",
    "    \\\n",
    "        \\underset{\\mathcal{Z}^{(1)}\\ \\in\\ \\mathbb{R}^{16\\times34}}{\n",
    "        \\begin{bmatrix}\n",
    "            z_{1,1} & \\ldots & z_{1,34}\\\\\n",
    "            \\vdots & \\ddots & \\vdots\\\\\n",
    "            z_{16,1} & \\ldots & z_{16,34}\n",
    "        \\end{bmatrix}}\n",
    "    \\\n",
    "    \\Biggr)\\\\\n",
    "    =\n",
    "    &\\quad\\quad\\quad\\underset{\\mathcal{A}^{(2)}\\ \\in\\ \\mathbb{R}^{16\\times34}}{\n",
    "    \\begin{bmatrix}\n",
    "        a_{1,1} & \\ldots & a_{1,34}\\\\\n",
    "        \\vdots & \\ddots & \\vdots\\\\\n",
    "        a_{16,1} & \\ldots & a_{16,34}\n",
    "    \\end{bmatrix}}\n",
    "    \\rightarrow\n",
    "    \\Biggl(\n",
    "    \\\n",
    "        \\underset{\\mathcal{A}^{(2)}\\ \\in\\ \\mathbb{R}^{16\\times34}}{\n",
    "        \\begin{bmatrix}\n",
    "            a_{1,1} & \\ldots & a_{1,34}\\\\\n",
    "            \\vdots & \\ddots & \\vdots\\\\\n",
    "            a_{16,1} & \\ldots & a_{16,34}\n",
    "        \\end{bmatrix}}\n",
    "    \\\n",
    "    \\Biggl)^{\\top}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### Layer 2\n",
    "$$\n",
    "\\begin{align}\n",
    "    &\\text{ReLU}\n",
    "    \\Biggl(\n",
    "    \\\n",
    "        \\underset{\\mathcal{W}^{(2)}\\ \\in\\ \\mathbb{R}^{16\\times16}}{\n",
    "        \\begin{bmatrix}\n",
    "            w_{1,1} & \\ldots & w_{1,16}\\\\\n",
    "            \\vdots & \\ddots & \\vdots\\\\\n",
    "            w_{16,1} & \\ldots & w_{16,16}\n",
    "        \\end{bmatrix}}\n",
    "        \\\n",
    "        \\Biggl(\n",
    "        \\\n",
    "            \\underset{\\hat{\\mathcal{A}}\\ \\in\\ \\mathbb{R}^{34\\times34}}{\n",
    "            \\begin{bmatrix}\n",
    "                \\alpha_{1,1} & \\ldots & \\alpha_{1,34}\\\\\n",
    "                \\vdots & \\ddots & \\vdots\\\\\n",
    "                \\alpha_{34,1} & \\ldots & \\alpha_{34,34}\n",
    "            \\end{bmatrix}}\n",
    "            \\\n",
    "            \\underset{\\mathcal{A}^{(2)^{{\\top}}}\\ \\in\\ \\mathbb{R}^{34\\times16}}{\n",
    "            \\begin{bmatrix}\n",
    "                a_{1,1} & \\ldots & a_{1,16}\\\\\n",
    "                \\vdots & \\ddots & \\vdots\\\\\n",
    "                a_{34,1} & \\ldots & a_{34,16}\n",
    "            \\end{bmatrix}}\n",
    "        \\\n",
    "        \\Biggl)^{\\top}\n",
    "        +\n",
    "        \\underset{\\mathcal{b}^{(2)}\\ \\in\\ \\mathbb{R}^{16\\times34}}{\n",
    "        \\begin{bmatrix}\n",
    "            b_{1,1} & \\ldots & b_{1,34}\\\\\n",
    "            \\vdots & \\ddots & \\vdots\\\\\n",
    "            b_{16,1} & \\ldots & b_{16,34}\n",
    "        \\end{bmatrix}}\n",
    "    \\\n",
    "    \\Biggr)\\\\\n",
    "    =\\\n",
    "    &\\text{ReLU}\n",
    "    \\Biggl(\n",
    "    \\\n",
    "        \\underset{\\mathcal{Z}^{(2)}\\ \\in\\ \\mathbb{R}^{16\\times34}}{\n",
    "        \\begin{bmatrix}\n",
    "            z_{1,1} & \\ldots & z_{1,34}\\\\\n",
    "            \\vdots & \\ddots & \\vdots\\\\\n",
    "            z_{16,1} & \\ldots & z_{16,34}\n",
    "        \\end{bmatrix}}\n",
    "    \\\n",
    "    \\Biggr)\\\\\n",
    "    =\n",
    "    &\\quad\\quad\\quad\\underset{\\mathcal{A}^{(3)}\\ \\in\\ \\mathbb{R}^{16\\times34}}{\n",
    "    \\begin{bmatrix}\n",
    "        a_{1,1} & \\ldots & a_{1,34}\\\\\n",
    "        \\vdots & \\ddots & \\vdots\\\\\n",
    "        a_{16,1} & \\ldots & a_{16,34}\n",
    "    \\end{bmatrix}}\n",
    "    \\rightarrow\n",
    "    \\Biggl(\n",
    "    \\\n",
    "        \\underset{\\mathcal{A}^{(3)}\\ \\in\\ \\mathbb{R}^{16\\times34}}{\n",
    "        \\begin{bmatrix}\n",
    "            a_{1,1} & \\ldots & a_{1,34}\\\\\n",
    "            \\vdots & \\ddots & \\vdots\\\\\n",
    "            a_{16,1} & \\ldots & a_{16,34}\n",
    "        \\end{bmatrix}}\n",
    "    \\Biggl)^{\\top}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "#### Layer 3\n",
    "$$\n",
    "\\begin{align}\n",
    "    &\\text{Softmax}\n",
    "    \\Biggl(\n",
    "    \\\n",
    "        \\underset{\\mathcal{W}^{(3)}\\ \\in\\ \\mathbb{R}^{3\\times16}}{\n",
    "        \\begin{bmatrix}\n",
    "            w_{1,1} & \\ldots & w_{1,16}\\\\\n",
    "            w_{2,1} & \\ldots & w_{2,16}\\\\\n",
    "            w_{3,1} & \\ldots & w_{3,16}\n",
    "        \\end{bmatrix}}\n",
    "        \\\n",
    "        \\Biggl(\n",
    "        \\\n",
    "            \\underset{\\hat{\\mathcal{A}}\\ \\in\\ \\mathbb{R}^{34\\times34}}{\n",
    "            \\begin{bmatrix}\n",
    "                \\alpha_{1,1} & \\ldots & \\alpha_{1,34}\\\\\n",
    "                \\vdots & \\ddots & \\vdots\\\\\n",
    "                \\alpha_{34,1} & \\ldots & \\alpha_{34,34}\n",
    "            \\end{bmatrix}}\n",
    "            \\ \n",
    "            \\underset{\\mathcal{A}^{(3)^{\\top}}\\ \\in\\ \\mathbb{R}^{34\\times16}}{\n",
    "            \\begin{bmatrix}\n",
    "                a_{1,1} & \\ldots & a_{1,16}\\\\\n",
    "                \\vdots & \\ddots & \\vdots\\\\\n",
    "                a_{34,1} & \\ldots & a_{34,16}\n",
    "            \\end{bmatrix}}\n",
    "        \\Biggl)^{\\top}\n",
    "        +\n",
    "        \\underset{\\mathcal{b}^{(3)}\\ \\in\\ \\mathbb{R}^{3\\times34}}{\n",
    "        \\begin{bmatrix}\n",
    "            b_{1,1} & \\ldots & b_{1,34}\\\\\n",
    "            b_{2,1} & \\ldots & b_{2,34}\\\\\n",
    "            b_{3,1} & \\ldots & b_{3,34}\n",
    "        \\end{bmatrix}}\n",
    "    \\\n",
    "    \\Biggr)\\\\\n",
    "    =\\\n",
    "    &\\text{Softmax}\n",
    "    \\Biggl(\n",
    "    \\\n",
    "        \\underset{\\mathcal{Z}^{(3)}\\ \\in\\ \\mathbb{R}^{3\\times34}}{\n",
    "        \\begin{bmatrix}\n",
    "            z_{1,1} & \\ldots & z_{1,34}\\\\\n",
    "            z_{2,1} & \\ldots & z_{2,34}\\\\\n",
    "            z_{3,1} & \\ldots & z_{3,34}\n",
    "        \\end{bmatrix}}\n",
    "    \\\n",
    "    \\Biggr)\\\\\n",
    "    =\n",
    "    &\\quad\\quad\\quad\\quad\\underset{\\mathcal{A}^{(4)}\\ \\in\\ \\mathbb{R}^{3\\times34}}{\n",
    "    \\begin{bmatrix}\n",
    "        a_{1,1} & \\ldots & a_{1,34}\\\\\n",
    "        a_{2,1} & \\ldots & a_{2,34}\\\\\n",
    "        a_{3,1} & \\ldots & a_{3,34}\n",
    "    \\end{bmatrix}}\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b928f54c",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "Why we use cross entropy loss for classification when doing MLE:\n",
    "https://en.wikipedia.org/wiki/Cross_entropy#Relation_to_maximum_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "886ecdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_ent(predictions, targets):\n",
    "    N = predictions.shape[1] # (3,34), so index 1 for samples\n",
    "    targets_ = np.squeeze(np.asarray(targets))\n",
    "    predictions_ = np.squeeze(np.asarray(predictions))\n",
    "    ce = -np.sum(targets_*np.log(predictions_))/N\n",
    "    return ce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7da7eda",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "#### Cross entropy loss\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\delta^{(4)}=&\\quad\\frac{\\partial}{\\partial z^{(3)}}\\ \\frac{1}{2} \\Big\\lVert Y-H_{\\mathcal{W},\\mathcal{b}}(\\mathcal{X})\\Big\\rVert^{2}\\\\\n",
    "    =&\\quad\\mathcal{A}^{(4)}-Y\\\\\n",
    "    =&\\underset{\\mathcal{A}^{(4)}\\ \\in\\ \\mathbb{R}^{3\\times34}}{\n",
    "    \\begin{bmatrix}\n",
    "        a_{1,1} & \\ldots & a_{1,34}\\\\\n",
    "        a_{2,1} & \\ldots & a_{2,34}\\\\\n",
    "        a_{3,1} & \\ldots & a_{3,34}\n",
    "    \\end{bmatrix}}\n",
    "    -\n",
    "    \\underset{\\mathcal{Y}^{(4)}\\ \\in\\ \\mathbb{R}^{3\\times34}}{\n",
    "    \\begin{bmatrix}\n",
    "        y_{1,1} & \\ldots & y_{1,34}\\\\\n",
    "        y_{2,1} & \\ldots & y_{2,34}\\\\\n",
    "        y_{3,1} & \\ldots & y_{3,34}\n",
    "    \\end{bmatrix}}\\\\\n",
    "    =&\\underset{\\mathcal{\\delta}^{(4)}\\ \\in\\ \\mathbb{R}^{3\\times34}}{\n",
    "    \\begin{bmatrix}\n",
    "        d_{1,1} & \\ldots & d_{1,34}\\\\\n",
    "        d_{2,1} & \\ldots & d_{2,34}\\\\\n",
    "        d_{3,1} & \\ldots & d_{3,34}\n",
    "    \\end{bmatrix}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### Layer 3\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\nabla W^{(3)} =& \\delta^{(4)}A^{(3)^{\\top}}\\\\\n",
    "    =& \n",
    "    \\underset{\\mathcal{\\delta}^{(4)}\\ \\in\\ \\mathbb{R}^{3\\times34}}{\n",
    "    \\begin{bmatrix}\n",
    "        d_{1,1} & \\ldots & d_{1,34}\\\\\n",
    "        d_{2,1} & \\ldots & d_{2,34}\\\\\n",
    "        d_{3,1} & \\ldots & d_{3,34}\n",
    "    \\end{bmatrix}}\n",
    "    \\\n",
    "    \\underset{\\mathcal{A}^{(3)^{\\top}}\\ \\in\\ \\mathbb{R}^{34\\times16}}{\n",
    "    \\begin{bmatrix}\n",
    "        a_{1,1} & \\ldots & a_{1,16}\\\\\n",
    "        \\vdots & \\ddots & \\vdots\\\\\n",
    "        a_{34,1} & \\ldots & a_{34,16}\n",
    "    \\end{bmatrix}}\\\\\n",
    "    =&\n",
    "    \\underset{\\nabla\\mathcal{W}^{(3)}\\ \\in\\ \\mathbb{R}^{3\\times16}}{\n",
    "    \\begin{bmatrix}\n",
    "        w_{1,1} & \\ldots & w_{1,16}\\\\\n",
    "        w_{2,1} & \\ldots & w_{2,16}\\\\\n",
    "        w_{3,1} & \\ldots & w_{3,16}\n",
    "    \\end{bmatrix}}\\\\\n",
    "    \\nabla b^{(3)}=&\\delta^{(4)}\\\\\n",
    "    =&\\underset{\\mathcal{\\delta}^{(4)}\\ \\in\\ \\mathbb{R}^{3\\times34}}{\n",
    "    \\begin{bmatrix}\n",
    "        d_{1,1} & \\ldots & d_{1,34}\\\\\n",
    "        d_{2,1} & \\ldots & d_{2,34}\\\\\n",
    "        d_{3,1} & \\ldots & d_{3,34}\n",
    "    \\end{bmatrix}}\\\\\n",
    "    \\delta^{(3)} =&\\mathcal{W}^{(3)^{\\top}}\\delta^{(4)}\\odot\\frac{\\partial}{\\partial z^{(2)}}\\text{ReLU}(z^{(2)})\\\\\n",
    "    =&\n",
    "    \\underset{\\mathcal{W}^{(3)^{\\top}}\\ \\in\\ \\mathbb{R}^{16\\times3}}{\n",
    "    \\begin{bmatrix}\n",
    "        w_{1,1} & w_{1,2} & w_{1,3}\\\\\n",
    "        \\vdots & \\ddots & \\vdots\\\\\n",
    "        w_{16,1} & w_{16,2} & w_{16,3}\n",
    "    \\end{bmatrix}}\n",
    "    \\\n",
    "    \\underset{\\mathcal{\\delta}^{(4)}\\ \\in\\ \\mathbb{R}^{3\\times34}}{\n",
    "    \\begin{bmatrix}\n",
    "        d_{1,1} & \\ldots & d_{1,34}\\\\\n",
    "        d_{2,1} & \\ldots & d_{2,34}\\\\\n",
    "        d_{3,1} & \\ldots & d_{3,34}\n",
    "    \\end{bmatrix}}\n",
    "    \\odot\n",
    "    \\frac{\\partial}{\\partial z^{(2)}}\n",
    "    \\text{ReLU}\n",
    "    \\Biggl(\n",
    "    \\ \n",
    "        \\underset{\\mathcal{Z}^{(2)}\\ \\in\\ \\mathbb{R}^{16\\times34}}{\n",
    "        \\begin{bmatrix}\n",
    "            z_{1,1} & \\ldots & z_{1,34}\\\\\n",
    "            \\vdots & \\ddots & \\vdots\\\\\n",
    "            z_{16,1} & \\ldots & z_{16,34}\n",
    "        \\end{bmatrix}}\n",
    "    \\ \\Biggl)\\\\\n",
    "    =&\n",
    "    \\underset{\\mathcal{\\delta}^{(3)}\\ \\in\\ \\mathbb{R}^{16\\times34}}{\n",
    "    \\begin{bmatrix}\n",
    "        d_{1,1} & \\ldots & d_{1,34}\\\\\n",
    "        \\vdots & \\ddots & \\vdots\\\\\n",
    "        d_{16,1} & \\ldots & d_{16,34}\n",
    "    \\end{bmatrix}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### Layer 2\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\nabla W^{(2)} =& \\delta^{(3)}A^{(2)^{\\top}}\\\\\n",
    "    =&\n",
    "    \\underset{\\mathcal{\\delta}^{(3)}\\ \\in\\ \\mathbb{R}^{16\\times34}}{\n",
    "    \\begin{bmatrix}\n",
    "        d_{1,1} & \\ldots & d_{1,34}\\\\\n",
    "        \\vdots & \\ddots & \\vdots\\\\\n",
    "        d_{16,1} & \\ldots & d_{16,34}\n",
    "    \\end{bmatrix}}\n",
    "    \\\n",
    "    \\underset{\\mathcal{A}^{(2)^{{\\top}}}\\ \\in\\ \\mathbb{R}^{34\\times16}}{\n",
    "            \\begin{bmatrix}\n",
    "                a_{1,1} & \\ldots & a_{1,16}\\\\\n",
    "                \\vdots & \\ddots & \\vdots\\\\\n",
    "                a_{34,1} & \\ldots & a_{34,16}\n",
    "            \\end{bmatrix}}\\\\\n",
    "    \\nabla b^{(2)} =& \\delta^{(3)}\\\\\n",
    "    =&\n",
    "    \\underset{\\mathcal{\\delta}^{(3)}\\ \\in\\ \\mathbb{R}^{16\\times34}}{\n",
    "    \\begin{bmatrix}\n",
    "        d_{1,1} & \\ldots & d_{1,34}\\\\\n",
    "        \\vdots & \\ddots & \\vdots\\\\\n",
    "        d_{16,1} & \\ldots & d_{16,34}\n",
    "    \\end{bmatrix}}\\\\\n",
    "    \\delta^{(2)} =&\\mathcal{W}^{(2)^{\\top}}\\delta^{(3)}\\odot\\frac{\\partial}{\\partial z^{(1)}}\\text{ReLU}(z^{(1)})\\\\\n",
    "    =&\n",
    "    \\underset{\\mathcal{W}^{(2)^{\\top}}\\ \\in\\ \\mathbb{R}^{16\\times16}}{\n",
    "    \\begin{bmatrix}\n",
    "        w_{1,1} & \\ldots & w_{1,16}\\\\\n",
    "        \\vdots & \\ddots & \\vdots\\\\\n",
    "        w_{16,1} & \\ldots & w_{16,16}\n",
    "    \\end{bmatrix}}\n",
    "    \\\n",
    "    \\underset{\\mathcal{\\delta}^{(3)}\\ \\in\\ \\mathbb{R}^{16\\times34}}{\n",
    "    \\begin{bmatrix}\n",
    "        d_{1,1} & \\ldots & d_{1,34}\\\\\n",
    "        \\vdots & \\ddots & \\vdots\\\\\n",
    "        d_{16,1} & \\ldots & d_{16,34}\n",
    "    \\end{bmatrix}}\n",
    "    \\\n",
    "    \\odot\\frac{\\partial}{\\partial z^{(1)}}\\text{ReLU}\n",
    "    \\Biggl(\n",
    "    \\\n",
    "        \\underset{\\mathcal{Z}^{(1)}\\ \\in\\ \\mathbb{R}^{16\\times34}}{\n",
    "        \\begin{bmatrix}\n",
    "            z_{1,1} & \\ldots & z_{1,34}\\\\\n",
    "            \\vdots & \\ddots & \\vdots\\\\\n",
    "            z_{16,1} & \\ldots & z_{16,34}\n",
    "        \\end{bmatrix}}\n",
    "    \\\n",
    "    \\Biggl)\\\\\n",
    "    =&\n",
    "    \\underset{\\mathcal{\\delta}^{(2)}\\ \\in\\ \\mathbb{R}^{16\\times34}}{\n",
    "    \\begin{bmatrix}\n",
    "        d_{1,1} & \\ldots & d_{1,34}\\\\\n",
    "        \\vdots & \\ddots & \\vdots\\\\\n",
    "        d_{16,1} & \\ldots & d_{16,34}\n",
    "    \\end{bmatrix}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### Layer 1\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\nabla W^{(1)} =& \\delta^{(2)}\\mathcal{X}^{\\top}\\\\\n",
    "    =&\n",
    "    \\underset{\\mathcal{\\delta}^{(2)}\\ \\in\\ \\mathbb{R}^{16\\times34}}{\n",
    "    \\begin{bmatrix}\n",
    "        d_{1,1} & \\ldots & d_{1,34}\\\\\n",
    "        \\vdots & \\ddots & \\vdots\\\\\n",
    "        d_{16,1} & \\ldots & d_{16,34}\n",
    "    \\end{bmatrix}}\n",
    "    \\ \n",
    "    \\underset{\\mathcal{X^{\\top}}\\ \\in\\ \\mathbb{R}^{34\\times34}}{\n",
    "            \\begin{bmatrix}\n",
    "                x_{1,1} & \\ldots & x_{1,34}\\\\\n",
    "                \\vdots & \\ddots & \\vdots\\\\\n",
    "                x_{34,1} & \\ldots & x_{34,34}\n",
    "            \\end{bmatrix}}\\\\\n",
    "    \\nabla b^{(1)} =& \\delta^{(2)}\\\\\n",
    "    =& \\underset{\\mathcal{\\delta}^{(2)}\\ \\in\\ \\mathbb{R}^{16\\times34}}{\n",
    "    \\begin{bmatrix}\n",
    "        d_{1,1} & \\ldots & d_{1,34}\\\\\n",
    "        \\vdots & \\ddots & \\vdots\\\\\n",
    "        d_{16,1} & \\ldots & d_{16,34}\n",
    "    \\end{bmatrix}}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5ac871",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e1a4f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss, epochs, features, labels, opt):\n",
    "    for e in range(epochs):\n",
    "        output = model(features)\n",
    "        loss_val = loss(output, labels)\n",
    "        deriv_loss = output - labels\n",
    "\n",
    "        model.backward(deriv_loss)\n",
    "        opt.step()\n",
    "        opt.zero_gradients()\n",
    "        \n",
    "        if e % 100 == 0:\n",
    "            print(f'(epoch {e}) loss: {loss_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f408834d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(epoch 0) loss: 1.457359485959475\n",
      "(epoch 100) loss: 0.9644013670396726\n",
      "(epoch 200) loss: 0.8344526653668007\n",
      "(epoch 300) loss: 0.6162441597634254\n",
      "(epoch 400) loss: 0.38847855008430154\n",
      "(epoch 500) loss: 0.25692153448299365\n",
      "(epoch 600) loss: 0.18825798022154486\n",
      "(epoch 700) loss: 0.14660657276145744\n",
      "(epoch 800) loss: 0.11771845053627594\n",
      "(epoch 900) loss: 0.09607806338910253\n",
      "(epoch 1000) loss: 0.07917886589226987\n",
      "(epoch 1100) loss: 0.06564813589999302\n",
      "(epoch 1200) loss: 0.05465303160533518\n",
      "(epoch 1300) loss: 0.04566636953432422\n",
      "(epoch 1400) loss: 0.0383219537459145\n",
      "(epoch 1500) loss: 0.032335732767277914\n",
      "(epoch 1600) loss: 0.027469138827650837\n",
      "(epoch 1700) loss: 0.02351424605367761\n",
      "(epoch 1800) loss: 0.02029347332665692\n",
      "(epoch 1900) loss: 0.01765977130887136\n",
      "(epoch 2000) loss: 0.015493663265433562\n",
      "(epoch 2100) loss: 0.01369981819075705\n",
      "(epoch 2200) loss: 0.012203054189802545\n",
      "(epoch 2300) loss: 0.010946791830738654\n",
      "(epoch 2400) loss: 0.009881523922433471\n",
      "(epoch 2500) loss: 0.008971784694204543\n",
      "(epoch 2600) loss: 0.008189252007617314\n",
      "(epoch 2700) loss: 0.007511511045027632\n",
      "(epoch 2800) loss: 0.006920721843892445\n",
      "(epoch 2900) loss: 0.006402592514844871\n",
      "(epoch 3000) loss: 0.005945595647889735\n",
      "(epoch 3100) loss: 0.005540781342545673\n",
      "(epoch 3200) loss: 0.005180102916760186\n",
      "(epoch 3300) loss: 0.0048571081943988665\n",
      "(epoch 3400) loss: 0.004566644440582126\n",
      "(epoch 3500) loss: 0.004304385445404245\n",
      "(epoch 3600) loss: 0.004066692275040406\n",
      "(epoch 3700) loss: 0.0038504982324147554\n",
      "(epoch 3800) loss: 0.0036532036645264758\n",
      "(epoch 3900) loss: 0.0034725933025275386\n",
      "(epoch 4000) loss: 0.003306769751807094\n",
      "(epoch 4100) loss: 0.00315410334777254\n",
      "(epoch 4200) loss: 0.0030131824185109955\n",
      "(epoch 4300) loss: 0.0028827827921217583\n",
      "(epoch 4400) loss: 0.002761838486570793\n",
      "(epoch 4500) loss: 0.0026494173264234766\n",
      "(epoch 4600) loss: 0.0025447011565353445\n",
      "(epoch 4700) loss: 0.0024469694104476097\n",
      "(epoch 4800) loss: 0.0023555906534572547\n",
      "(epoch 4900) loss: 0.002269992016455352\n",
      "(epoch 5000) loss: 0.0021896758994184727\n",
      "(epoch 5100) loss: 0.00211419522301303\n",
      "(epoch 5200) loss: 0.002043148353582368\n",
      "(epoch 5300) loss: 0.001976176755134958\n",
      "(epoch 5400) loss: 0.0019129582534635702\n",
      "(epoch 5500) loss: 0.0018532026505713433\n",
      "(epoch 5600) loss: 0.0017966479383657715\n",
      "(epoch 5700) loss: 0.0017430570336362185\n",
      "(epoch 5800) loss: 0.001692215635809392\n",
      "(epoch 5900) loss: 0.0016439288658147005\n",
      "(epoch 6000) loss: 0.0015980164556752311\n",
      "(epoch 6100) loss: 0.0015543178869098465\n",
      "(epoch 6200) loss: 0.0015126853324486534\n",
      "(epoch 6300) loss: 0.0014729823949914188\n",
      "(epoch 6400) loss: 0.0014350844209047265\n",
      "(epoch 6500) loss: 0.0013988770684040296\n",
      "(epoch 6600) loss: 0.0013642552075777256\n",
      "(epoch 6700) loss: 0.0013311220570794476\n",
      "(epoch 6800) loss: 0.0012993883321820703\n",
      "(epoch 6900) loss: 0.0012689713646664264\n",
      "(epoch 7000) loss: 0.0012397948089929959\n",
      "(epoch 7100) loss: 0.001211787935587957\n",
      "(epoch 7200) loss: 0.0011848851352509872\n",
      "(epoch 7300) loss: 0.00115902547427283\n",
      "(epoch 7400) loss: 0.0011341522940010113\n",
      "(epoch 7500) loss: 0.001110212849985743\n",
      "(epoch 7600) loss: 0.0010871579864024424\n",
      "(epoch 7700) loss: 0.0010649418419079843\n",
      "(epoch 7800) loss: 0.0010435215696650876\n",
      "(epoch 7900) loss: 0.001022857127813007\n",
      "(epoch 8000) loss: 0.0010029110659909443\n",
      "(epoch 8100) loss: 0.0009836482675271972\n",
      "(epoch 8200) loss: 0.0009650358092967967\n",
      "(epoch 8300) loss: 0.0009470427945881342\n",
      "(epoch 8400) loss: 0.0009296402030470015\n",
      "(epoch 8500) loss: 0.0009128007468604213\n",
      "(epoch 8600) loss: 0.0008964987525297645\n",
      "(epoch 8700) loss: 0.0008807100421917142\n",
      "(epoch 8800) loss: 0.0008654118283642158\n",
      "(epoch 8900) loss: 0.0008505826172588389\n",
      "(epoch 9000) loss: 0.0008362021199171368\n",
      "(epoch 9100) loss: 0.0008222511704672277\n",
      "(epoch 9200) loss: 0.0008087116508564192\n",
      "(epoch 9300) loss: 0.0007955664214759686\n",
      "(epoch 9400) loss: 0.0007827992571536746\n",
      "(epoch 9500) loss: 0.0007703947880409331\n",
      "(epoch 9600) loss: 0.0007583384449668074\n",
      "(epoch 9700) loss: 0.0007466164088754762\n",
      "(epoch 9800) loss: 0.0007352155639980716\n",
      "(epoch 9900) loss: 0.000724123454444425\n",
      "(epoch 10000) loss: 0.0007133282439302194\n",
      "(epoch 10100) loss: 0.0007028186783803016\n",
      "(epoch 10200) loss: 0.0006925840511747811\n",
      "(epoch 10300) loss: 0.0006826141708234023\n",
      "(epoch 10400) loss: 0.0006728993308754941\n",
      "(epoch 10500) loss: 0.0006634302818886022\n",
      "(epoch 10600) loss: 0.0006541982052945722\n",
      "(epoch 10700) loss: 0.0006451946890169771\n",
      "(epoch 10800) loss: 0.0006364117047052154\n",
      "(epoch 10900) loss: 0.0006278415864632328\n",
      "(epoch 11000) loss: 0.0006194770109603327\n",
      "(epoch 11100) loss: 0.0006113109788216358\n",
      "(epoch 11200) loss: 0.0006033367973021886\n",
      "(epoch 11300) loss: 0.00059554806379098\n",
      "(epoch 11400) loss: 0.0005879386503742516\n",
      "(epoch 11500) loss: 0.0005805026898846328\n",
      "(epoch 11600) loss: 0.0005732345622094425\n",
      "(epoch 11700) loss: 0.0005661288816358242\n",
      "(epoch 11800) loss: 0.0005591804849548802\n",
      "(epoch 11900) loss: 0.0005523844202733977\n",
      "(epoch 12000) loss: 0.0005457359364855978\n",
      "(epoch 12100) loss: 0.000539230473360517\n",
      "(epoch 12200) loss: 0.0005328636522038373\n",
      "(epoch 12300) loss: 0.0005266312670565218\n",
      "(epoch 12400) loss: 0.0005205292763942297\n",
      "(epoch 12500) loss: 0.0005145537952956663\n",
      "(epoch 12600) loss: 0.0005087010880488468\n",
      "(epoch 12700) loss: 0.0005029675611678732\n",
      "(epoch 12800) loss: 0.000497349741824765\n",
      "(epoch 12900) loss: 0.0004918442779115978\n",
      "(epoch 13000) loss: 0.0004864480224848096\n",
      "(epoch 13100) loss: 0.0004811578800732461\n",
      "(epoch 13200) loss: 0.0004759708715582745\n",
      "(epoch 13300) loss: 0.00047088412746170577\n",
      "(epoch 13400) loss: 0.00046589488228386994\n",
      "(epoch 13500) loss: 0.0004610004695050224\n",
      "(epoch 13600) loss: 0.0004561983170433426\n",
      "(epoch 13700) loss: 0.00045148592965167504\n",
      "(epoch 13800) loss: 0.0004468608195959569\n",
      "(epoch 13900) loss: 0.0004423208398849392\n",
      "(epoch 14000) loss: 0.00043786373053585916\n",
      "(epoch 14100) loss: 0.0004334873186961549\n",
      "(epoch 14200) loss: 0.00042918951050478435\n",
      "(epoch 14300) loss: 0.0004249682853433667\n",
      "(epoch 14400) loss: 0.00042082169541638045\n",
      "(epoch 14500) loss: 0.00041674790178326566\n",
      "(epoch 14600) loss: 0.000412745025775126\n",
      "(epoch 14700) loss: 0.00040881130223548666\n",
      "(epoch 14800) loss: 0.0004049451341355316\n",
      "(epoch 14900) loss: 0.00040114474086128533\n",
      "(epoch 15000) loss: 0.00039740852157438847\n",
      "(epoch 15100) loss: 0.00039373493195492005\n",
      "(epoch 15200) loss: 0.0003901224739972883\n",
      "(epoch 15300) loss: 0.00038656969494531637\n",
      "(epoch 15400) loss: 0.00038307518601027454\n",
      "(epoch 15500) loss: 0.00037963758098531975\n",
      "(epoch 15600) loss: 0.0003762555548229108\n",
      "(epoch 15700) loss: 0.0003729278222142714\n",
      "(epoch 15800) loss: 0.00036965313619321334\n",
      "(epoch 15900) loss: 0.0003664302867768105\n",
      "(epoch 16000) loss: 0.0003632580996497585\n",
      "(epoch 16100) loss: 0.0003601354348953469\n",
      "(epoch 16200) loss: 0.0003570611857743033\n",
      "(epoch 16300) loss: 0.00035403427755122016\n",
      "(epoch 16400) loss: 0.00035105366636760757\n",
      "(epoch 16500) loss: 0.0003481183381602048\n",
      "(epoch 16600) loss: 0.000345227307622917\n",
      "(epoch 16700) loss: 0.00034237961721065755\n",
      "(epoch 16800) loss: 0.0003395743361837049\n",
      "(epoch 16900) loss: 0.0003368105596900958\n",
      "(epoch 17000) loss: 0.00033408740788516537\n",
      "(epoch 17100) loss: 0.00033140404282958774\n",
      "(epoch 17200) loss: 0.0003287596887375106\n",
      "(epoch 17300) loss: 0.000326153437198196\n",
      "(epoch 17400) loss: 0.0003235845084229443\n",
      "(epoch 17500) loss: 0.00032105214089309336\n",
      "(epoch 17600) loss: 0.00031855559421187555\n",
      "(epoch 17700) loss: 0.0003160941508731515\n",
      "(epoch 17800) loss: 0.0003136670949990597\n",
      "(epoch 17900) loss: 0.00031127373841198936\n",
      "(epoch 18000) loss: 0.0003089134105705977\n",
      "(epoch 18100) loss: 0.00030658545808109845\n",
      "(epoch 18200) loss: 0.00030428924420426023\n",
      "(epoch 18300) loss: 0.0003020241483655333\n",
      "(epoch 18400) loss: 0.0002997895656735172\n",
      "(epoch 18500) loss: 0.000297584906448558\n",
      "(epoch 18600) loss: 0.00029540959576451547\n",
      "(epoch 18700) loss: 0.0002932630730035889\n",
      "(epoch 18800) loss: 0.000291144791425056\n",
      "(epoch 18900) loss: 0.00028905421774797803\n",
      "(epoch 19000) loss: 0.0002869908317478733\n",
      "(epoch 19100) loss: 0.0002849541258667369\n",
      "(epoch 19200) loss: 0.0002829436048365604\n",
      "(epoch 19300) loss: 0.00028096572143787167\n",
      "(epoch 19400) loss: 0.0002790126877885006\n",
      "(epoch 19500) loss: 0.0002770840072069214\n",
      "(epoch 19600) loss: 0.00027517936211135345\n",
      "(epoch 19700) loss: 0.0002732983994668341\n",
      "(epoch 19800) loss: 0.00027144063027402217\n",
      "(epoch 19900) loss: 0.0002696057875933275\n"
     ]
    }
   ],
   "source": [
    "features = np.eye(num_nodes)\n",
    "model = GCN(A_hat, num_classes)\n",
    "opt = GradientDescent(model.parameters, lr)\n",
    "train(model, cross_ent, epochs, features, labels, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b937ff89",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nodes = np.array([0,1,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a861a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d962ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
