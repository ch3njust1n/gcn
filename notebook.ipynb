{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "1. Softmax layer\n",
    "2. Fix backprop\n",
    "3. Gradient checking\n",
    "4. Visualize loss\n",
    "5. Embeddings\n",
    "6. Gradient of cross ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import argparse\n",
    "import random\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from pyvis.network import Network\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.algorithms.community.modularity_max import greedy_modularity_communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 100\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.karate_club_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34, 78)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.number_of_nodes(), G.number_of_edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate labels from communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "communities = greedy_modularity_communities(G)\n",
    "colors = np.zeros(G.number_of_nodes())\n",
    "classes = set()\n",
    "\n",
    "for i, c in enumerate(communities):\n",
    "    colors[list(c)] = i\n",
    "    classes.add(i)\n",
    "    \n",
    "num_classes = len(classes)\n",
    "labels = np.eye(len(classes))[colors.astype(int)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"500px\"\n",
       "            src=\"colored_graph.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fd6601e7520>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def random_color():\n",
    "    return '#%02X%02X%02X' % (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
    "\n",
    "# uncomment for random colors\n",
    "# color_map = {cls: random_color() for cls in classes}\n",
    "color_map = {0: '#46FB47', 1: '#B9E6B5', 2: '#9F9EBF'}\n",
    "\n",
    "colored_graph = Network(width='100%', notebook=True)\n",
    "\n",
    "for node in G.nodes():\n",
    "    colored_graph.add_node(node, color=color_map[int(colors[node])])\n",
    "    \n",
    "for edge in G.edges():\n",
    "    colored_graph.add_edge(int(edge[0]), int(edge[1]))\n",
    "    \n",
    "colored_graph.show('colored_graph.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def renormalization(G):\n",
    "    A = nx.to_numpy_matrix(G)\n",
    "    I = np.eye(len(A))\n",
    "    A_tilde = A + I\n",
    "    D_tilde = np.zeros(A.shape, int)\n",
    "    np.fill_diagonal(D_tilde, np.sum(A_tilde, axis=1).flatten())\n",
    "    D_tilde = np.linalg.inv(D_tilde)\n",
    "    D_tilde = np.power(D_tilde, 0.5)\n",
    "    return D_tilde @ A_tilde @ D_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 1., 1., ..., 1., 0., 0.],\n",
       "        [1., 0., 1., ..., 0., 0., 0.],\n",
       "        [1., 1., 0., ..., 0., 1., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 1., 1.],\n",
       "        [0., 0., 1., ..., 1., 0., 1.],\n",
       "        [0., 0., 0., ..., 1., 1., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.to_numpy_matrix(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05882353 0.0766965  0.07312724 ... 0.09166985 0.         0.        ]\n",
      " [0.0766965  0.1        0.09534626 ... 0.         0.         0.        ]\n",
      " [0.07312724 0.09534626 0.09090909 ... 0.         0.0836242  0.        ]\n",
      " ...\n",
      " [0.09166985 0.         0.         ... 0.14285714 0.10482848 0.08908708]\n",
      " [0.         0.         0.0836242  ... 0.10482848 0.07692308 0.06537205]\n",
      " [0.         0.         0.         ... 0.08908708 0.06537205 0.05555556]]\n"
     ]
    }
   ],
   "source": [
    "A_hat = renormalization(G)\n",
    "print(A_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glorot_init(in_dim, out_dim):\n",
    "    sd = np.sqrt(6.0 / (in_dim + out_dim))\n",
    "    return np.random.uniform(-sd, sd, size=(in_dim, out_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "For each iteration of gradient descent, compute local gradients with **backpropagation** - compute global gradient as a chain of local gradients using chain rule of calculus.\n",
    "\n",
    "Starting from the last layer $l$:\n",
    "\n",
    "$$\n",
    "\\delta^{(l)}=\\frac{\\partial}{\\partial z^{(l)}}\\mathcal{L}\n",
    "$$\n",
    "\n",
    "Think of the **error signal** $\\delta^{(l)}$ as serving as the same function as $x$ during the forward pass.\n",
    "\n",
    "For layer $i$ in $l-1$ to $2$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\nabla_{w^{(i)}}\\mathcal{L}&=\\delta^{(i+1)}a^{(i-1)^{\\top}}&&\\quad a^{(i-1)}\\text{ is the incoming activation to layer } i \\text{ from layer }i-1\\\\\\\\\n",
    "    \\nabla_{b^{(i)}}\\mathcal{L}&=\\delta^{(i+1)}&&\\quad\\text{shift by error signal}\\\\\\\\  \n",
    "    \\delta^{i} &= W^{i^{\\top}}\\delta^{i+1}\\odot\\frac{\\partial}{\\partial z}f(z^{(i-1)})&&\\quad W^{i^{\\top}} \\text{ propagates the error signal backwards as a linear combination scaled by the derivative}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Think about the convexity of the latent space as discretized across dimensions of $z\\in\\mathbb{R}^{N}$. Each row of a hidden layer corresponds to a neuron, which corresponds to a position in the activation vector for a given input vector. As $\\frac{\\partial}{\\partial z}f(z)\\rightarrow 0$, for the particular combination of activations given to this neuron from the previous layer, the derivative indicates how this specific neuron is changing - which measures the convexity of this particular dimension in latent space $\\mathbb{R}^{N}$.\n",
    "\n",
    "As $\\frac{\\partial}{\\partial z}f(z)\\rightarrow 0$, the neuron reacts less to the incoming linear combination, which means that for this combination of features, there is a local minima in this latent space. So as backpropagation progresses across training, gradients of neurons converge to local minima.\n",
    "\n",
    "For each layer $i$, apply gradient descent:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    W^{(i)} &= W^{(i)} - \\alpha\\mathbb{E}[\\nabla_{W^{(i)}}\\mathcal{L}]\\\\\n",
    "    b^{(i)} &= b^{(i)} - \\alpha\\mathbb{E}[\\nabla_{b^{(i)}}\\mathcal{L}]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\nabla_{W_{j}}]=\\frac{1}{b}\\sum^{b}_{j=1}\\nabla_{W_{j}}\\mathcal{L} \\text{ is the expected gradient of the batch of samples and same for bias}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent(object):\n",
    "    def __init__(self, parameters, learning_rate):\n",
    "        self.parameters = parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        \n",
    "    def zero_gradients(self):\n",
    "        for layer in self.parameters:\n",
    "            layer.W_grad = np.zeros(layer.W.shape)\n",
    "            layer.b_grad = np.zeros(layer.b.shape)\n",
    "    \n",
    "    \n",
    "    def step(self):\n",
    "        for layer in self.parameters:\n",
    "            layer.W -= self.learning_rate * layer.W_grad\n",
    "            layer.b -= self.learning_rate * layer.b_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Convolutional Layer\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(\\hat{A}XW^{1}+b^{1})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCLayer(object):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.W = glorot_init(output_dim, input_dim)\n",
    "        self.W_grad = np.zeros(self.W.shape)\n",
    "        self.b = np.ones((output_dim, 1))\n",
    "        self.b_grad = np.zeros(self.b.shape)\n",
    "        \n",
    "        \n",
    "    def relu(self, x):\n",
    "        return np.vectorize(lambda i : i if i > 0 else 0)(x)\n",
    "    \n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        return (x > 0) * 1\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    inputs:\n",
    "    G (nx.Graph)   Normalized Laplacian matrix for a static graph.\n",
    "                   Dimensions: N x N where N is the number of nodes.\n",
    "    x (np.ndarray) Embedding matrix\n",
    "                   Dimensions: N x F where F is the number of features.\n",
    "    '''\n",
    "    def __call__(self, G, x):\n",
    "        # (nodes x nodes), (nodes x features), so need to transpose\n",
    "        # before taking linear combination\n",
    "        self.i = x # (nxf)\n",
    "        self.X = (G @ x).T # (n,n) x (n,f) -> (n,f).T -> (f,n)\n",
    "        self.z = self.W @ self.X + self.b # (h,f) x (f,n) + (h,1) -> (h,n). Broadcast bias vector.\n",
    "        self.a = self.relu(self.z) # (h,n), where n is number of samples/nodes\n",
    "        \n",
    "        # print('GC Layer')\n",
    "        # print(f'x.shape: {x.shape}')\n",
    "        # print(f'X.shape: {self.X.shape}')\n",
    "        # print(f'W.shape: {self.W.shape}')\n",
    "        # print(f'b.shape: {self.b.shape}')\n",
    "        # print(f'z.shape: {self.z.shape}')\n",
    "        # print(f'a.shape: {self.a.shape}')\n",
    "        \n",
    "        # transpose so can multiply by adjacency matrix in next layer\n",
    "        return self.a.T # (n,h)\n",
    "    \n",
    "    \n",
    "    def backward(self, G, error):\n",
    "        samples = self.X.shape[0] # batch size\n",
    "        #should this be self.X or self.i?\n",
    "        print(f'GC backward\\nerror: {error.shape}\\tX.T: {self.X.T.shape}')\n",
    "        # TODO: Sum across non-batch axis\n",
    "        # TODO: Divide both dW and db before adding to self.W_grad and self.b_grad\n",
    "        self.W_grad += error @ self.X.T # (h,n) x (n,f) -> (h,f) which matches W.shape \n",
    "        self.b_grad += error # (h,n) \n",
    "        return self.W.T @ error * self.relu_derivative(self.z)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(object):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.W = glorot_init(output_dim, input_dim)\n",
    "        self.W_grad = np.zeros(self.W.shape)\n",
    "        self.b = np.ones((self.output_dim, 1))\n",
    "        self.b_grad = np.zeros(self.b.shape)\n",
    "        \n",
    "        \n",
    "    def relu(self, x):\n",
    "        return np.vectorize(lambda i : i if i > 0 else 0)(x)\n",
    "    \n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        return (x > 0) * 1\n",
    "    \n",
    "\n",
    "    '''\n",
    "    inputs:\n",
    "    x (np.ndarray) Inputs to this layer\n",
    "    \n",
    "    outputs:\n",
    "    a (np.ndarray) Output activations\n",
    "    '''\n",
    "    def __call__(self, x):    \n",
    "        self.x = x # (f,n)\n",
    "        self.z = self.W @ x + self.b # (h,f) x (f,n) + (h,1). Broadcast bias vector.\n",
    "        self.a = self.relu(self.z)   # (h,n)\n",
    "        \n",
    "        # print('Linear layer')\n",
    "        # print(f'x.shape: {x.shape}')\n",
    "        # print(f'W.shape: {self.W.shape}')\n",
    "        # print(f'b.shape: {self.b.shape}')\n",
    "        # print(f'z.shape: {self.z.shape}')\n",
    "        \n",
    "        return self.a.T # (h,n)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    inputs:\n",
    "    error (np.ndarray) Error signal of shape (W.out_dim, batch_size) from subsequent layer\n",
    "    \n",
    "    outputs:\n",
    "    \n",
    "    '''\n",
    "    def backward(self, error):\n",
    "        print(f'linear backward\\nerror: {error.shape}\\tX.T: {self.x.T.shape}')\n",
    "        batch_size = error.shape[1]\n",
    "        self.W_grad += (error @ self.x.T) / batch_size # (h,n) x (n,f)\n",
    "        self.b_grad += (np.sum(error, axis=1)) / batch_size # (h,n)\n",
    "        print(f'linear z: {self.z.shape}') #(3x34)\n",
    "        return self.W.T @ np.multiply(error, self.relu_derivative(self.z)).T # (f,h) x (h,n) * (h,n)\n",
    "        # (16,34) and (3,34)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Convolutional Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(object):\n",
    "    def __init__(self, graph, num_classes):\n",
    "        self.G = graph\n",
    "        self.nodes = self.G.shape[0]\n",
    "        self.embedding = np.eye(self.nodes)\n",
    "        self.l0 = GCLayer(self.nodes, 16)\n",
    "        self.l1 = GCLayer(16, 16)\n",
    "        self.l2 = Linear(16, num_classes)\n",
    "        self.parameters = [self.l0, self.l1, self.l2]\n",
    "        \n",
    "        \n",
    "    def softmax(self, x):\n",
    "        exps = np.exp(x - np.max(x))\n",
    "        return exps / np.sum(exps)\n",
    "        \n",
    "    \n",
    "    def __call__(self, x):\n",
    "        print(f'backward:\\nx.shape: {x.shape}')\n",
    "        a0 = self.l0(self.G, x)\n",
    "        # print(f'a0.shape: {a0.shape}')\n",
    "        a1 = self.l1(self.G, a0).T\n",
    "        a2 = self.l2(a1)\n",
    "        return self.softmax(a2)\n",
    "    \n",
    "    \n",
    "    def backward(self, x):\n",
    "        # Transpose errors from (34,3) -> (3,34) because linear weights are (16,34), but transposed for BP\n",
    "        # so computation must be (labels, batch_size) x (batch_size, hidden_dim) to get error @ x.T\n",
    "        d2 = self.l2.backward(x.T)\n",
    "        print(f'd2.shape: {d2.shape}')\n",
    "        d1 = self.l1.backward(d2)\n",
    "        print(f'd1.shape: {d1.shape}')\n",
    "        d0 = self.l0.backwards(d1)\n",
    "        print(f'd0.shape: {d0.shape}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_ent(predictions, targets):\n",
    "    # print(f'pred: {predictions.shape}')\n",
    "    # print(f'targets: {targets.shape}')\n",
    "    N = predictions.shape[0]\n",
    "    # print(f'log: {np.log(predictions).shape}')\n",
    "    targets_ = np.squeeze(np.asarray(targets))\n",
    "    predictions_ = np.squeeze(np.asarray(predictions))\n",
    "    print(f'a: {targets.shape}')\n",
    "    print(f'b: {predictions.shape}')\n",
    "    print(f'a: {targets_.shape}')\n",
    "    print(f'b: {predictions_.shape}')\n",
    "    ce = -np.sum(targets_*np.log(predictions_))/N\n",
    "    return ce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss, epochs, features, labels, opt):\n",
    "    for e in range(epochs):\n",
    "        output = model(features)\n",
    "        # print(f'output: {output}')\n",
    "        loss = loss(output, labels)\n",
    "        deriv_loss = output - labels\n",
    "        \n",
    "        print(f'loss: {loss}\\n{loss.shape}')\n",
    "        print(f'loss_: {deriv_loss}\\n{deriv_loss.shape}')\n",
    "        \n",
    "        model.backward(deriv_loss)\n",
    "        opt.step()\n",
    "        opt.zero_gradients()\n",
    "        \n",
    "        print(f'epoch {e} loss: {loss.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backward:\n",
      "x.shape: (34, 34)\n",
      "a: (34, 3)\n",
      "b: (34, 3)\n",
      "a: (34, 3)\n",
      "b: (34, 3)\n",
      "loss: 4.643994759236357\n",
      "()\n",
      "loss_: [[ 0.02279074  0.00442604 -0.98865356]\n",
      " [ 0.01648075 -0.9955144   0.01279877]\n",
      " [ 0.01740931 -0.99555317  0.01246242]\n",
      " [ 0.01358613 -0.99511489  0.01211563]\n",
      " [ 0.01188152  0.00510192 -0.9891818 ]\n",
      " [ 0.01327348  0.00492572 -0.98922424]\n",
      " [ 0.01286608  0.00487609 -0.98908503]\n",
      " [ 0.01172651 -0.99528044  0.01211342]\n",
      " [-0.98782429  0.00457422  0.0116606 ]\n",
      " [ 0.01178838 -0.99540219  0.01154992]\n",
      " [ 0.01196364  0.00498934 -0.98917667]\n",
      " [ 0.00957573  0.0052977  -0.98918376]\n",
      " [ 0.01097949 -0.99477815  0.01127291]\n",
      " [ 0.01185153 -0.99516203  0.01192877]\n",
      " [-0.98878699  0.00491238  0.01084445]\n",
      " [-0.98853434  0.00482882  0.01078431]\n",
      " [ 0.01200131  0.0048444  -0.98915279]\n",
      " [ 0.01036368 -0.99506526  0.01183964]\n",
      " [-0.98886031  0.00472971  0.01054909]\n",
      " [ 0.0105768   0.00490197 -0.98853167]\n",
      " [-0.98889423  0.00483625  0.011265  ]\n",
      " [ 0.0110712  -0.99539983  0.01183639]\n",
      " [-0.98860287  0.00455527  0.01142251]\n",
      " [-0.98610215  0.00451057  0.0113065 ]\n",
      " [-0.98724074  0.00447705  0.0117644 ]\n",
      " [-0.98743324  0.00441201  0.01169615]\n",
      " [-0.98830805  0.00479867  0.01071265]\n",
      " [-0.98726929  0.00454919  0.01150027]\n",
      " [-0.9887899   0.0046289   0.01204835]\n",
      " [-0.98690612  0.00464555  0.01081224]\n",
      " [-0.9881423   0.00456896  0.01169123]\n",
      " [-0.98644979  0.00450117  0.0119297 ]\n",
      " [-0.9786977   0.00418897  0.01147297]\n",
      " [-0.97151404  0.0040228   0.01114796]]\n",
      "(34, 3)\n",
      "linear backward\n",
      "error: (3, 34)\tX.T: (34, 16)\n",
      "linear z: (3, 34)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 34 is different from 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-62d6ee4b54ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGCN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradientDescent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_ent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-bf72f8585daf>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loss, epochs, features, labels, opt)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'loss_: {deriv_loss}\\n{deriv_loss.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mderiv_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-d2da1fcda390>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Transpose errors from (34,3) -> (3,34) because linear weights are (16,34), but transposed for BP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# so computation must be (labels, batch_size) x (batch_size, hidden_dim) to get error @ x.T\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0md2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'd2.shape: {d2.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0md1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-3f7ab8880a62>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, error)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_grad\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;31m# (h,n)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'linear z: {self.z.shape}'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(3x34)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;31m# (f,h) x (h,n) * (h,n)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;31m# (16,34) and (3,34)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 34 is different from 3)"
     ]
    }
   ],
   "source": [
    "features = np.eye(G.number_of_nodes())\n",
    "model = GCN(A_hat, num_classes)\n",
    "opt = GradientDescent(model.parameters, lr)\n",
    "train(model, cross_ent, epochs, features, labels, opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common errors\n",
    "\n",
    "Error:\n",
    "`ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 34 is different from 3)`\n",
    "\n",
    "Solution:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
