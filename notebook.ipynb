{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d000ebc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import argparse\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from pyvis.network import Network\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.algorithms.community.modularity_max import greedy_modularity_communities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b91a37",
   "metadata": {},
   "source": [
    "### Graph data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ff93e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.karate_club_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c173c85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.number_of_nodes(), G.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a843e431",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphvis = Network(notebook=True)\n",
    "graphvis.from_nx(G)\n",
    "graphvis.show('karate.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b3cee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.to_numpy_matrix(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ffb7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def renormalization(G):\n",
    "    A = nx.to_numpy_matrix(G)\n",
    "    I = np.eye(len(A))\n",
    "    A_tilde = A + I\n",
    "    D_tilde = np.zeros(A.shape, int)\n",
    "    np.fill_diagonal(D_tilde, np.sum(A_tilde, axis=1).flatten())\n",
    "    D_tilde = np.linalg.inv(D_tilde)\n",
    "    D_tilde = np.power(D_tilde, 0.5)\n",
    "    return D_tilde @ A_tilde @ D_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac45398",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_hat = renormalization(G)\n",
    "print(A_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8afcdc7",
   "metadata": {},
   "source": [
    "### Generate labels from communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bff161",
   "metadata": {},
   "outputs": [],
   "source": [
    "communities = greedy_modularity_communities(G)\n",
    "colors = np.zeros(G.number_of_nodes())\n",
    "classes = set()\n",
    "\n",
    "for i, c in enumerate(communities):\n",
    "    colors[list(c)] = i\n",
    "    classes.add(i)\n",
    "    \n",
    "labels = np.eye(len(classes))[colors.astype(int)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b067f04",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da59afd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0, keepdims=True)\n",
    "\n",
    "def glorot_init(in_dim, out_dim):\n",
    "    sd = np.sqrt(6.0 / (in_dim + out_dim))\n",
    "    return np.random.uniform(-sd, sd, size=(in_dim, out_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89d871b",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "For each iteration of gradient descent, compute local gradients with **backpropagation** - compute global gradient as a chain of local gradients using chain rule of calculus.\n",
    "\n",
    "Starting from the last layer $l$:\n",
    "\n",
    "$$\n",
    "\\delta^{(l)}=\\frac{\\partial}{\\partial z^{(l)}}\\mathcal{L}\n",
    "$$\n",
    "\n",
    "Think of the **error signal** $\\delta^{(l)}$ as serving as the same function as $x$ during the forward pass.\n",
    "\n",
    "For layer $i$ in $l-1$ to $2$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\nabla_{w^{(i)}}\\mathcal{L}&=\\delta^{(i+1)}a^{(i-1)^{\\top}}&&\\quad a^{(i-1)}\\text{ is the incoming activation to layer } i \\text{ from layer }i-1\\\\\\\\\n",
    "    \\nabla_{b^{(i)}}\\mathcal{L}&=\\delta^{(i+1)}&&\\quad\\text{shift by error signal}\\\\\\\\  \n",
    "    \\delta^{i} &= W^{i^{\\top}}\\delta^{i+1}\\odot\\frac{\\partial}{\\partial z}f(z^{(i-1)})&&\\quad W^{i^{\\top}} \\text{ propagates the error signal backwards as a linear combination scaled by the derivative}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Think about the convexity of the latent space as discretized across dimensions of $z\\in\\mathbb{R}^{N}$. Each row of a hidden layer corresponds to a neuron, which corresponds to a position in the activation vector for a given input vector. As $\\frac{\\partial}{\\partial z}f(z)\\rightarrow 0$, for the particular combination of activations given to this neuron from the previous layer, the derivative indicates how this specific neuron is changing - which measures the convexity of this particular dimension in latent space $\\mathbb{R}^{N}$.\n",
    "\n",
    "As $\\frac{\\partial}{\\partial z}f(z)\\rightarrow 0$, the neuron reacts less to the incoming linear combination, which means that for this combination of features, there is a local minima in this latent space. So as backpropagation progresses across training, gradients of neurons converge to local minima.\n",
    "\n",
    "For each layer $i$, apply gradient descent:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    W^{(i)} &= W^{(i)} - \\alpha\\mathbb{E}[\\nabla_{W^{(i)}}\\mathcal{L}]\\\\\n",
    "    b^{(i)} &= b^{(i)} - \\alpha\\mathbb{E}[\\nabla_{b^{(i)}}\\mathcal{L}]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\nabla_{W_{j}}]=\\frac{1}{b}\\sum^{b}_{j=1}\\nabla_{W_{j}}\\mathcal{L} \\text{ is the expected gradient of the batch of samples and same for bias}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4247a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent(object):\n",
    "    def __init__(self, parameters, learning_rate):\n",
    "        self.parameters = parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        \n",
    "    def zero_gradients(self):\n",
    "        for layer in self.parameters:\n",
    "            layer.W_grad = np.zeros(layer.W.shape)\n",
    "            layer.b_grad = np.zeros(layer.b.shape)\n",
    "    \n",
    "    \n",
    "    def step(self):\n",
    "        for layer in self.parameters:\n",
    "            layer.W -= self.learning_rate * layer.W_grad\n",
    "            layer.b -= self.learning_rate * layer.b_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27d15f1",
   "metadata": {},
   "source": [
    "### Graph Convolutional Layer\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(\\hat{A}XW^{1}+b^{1})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1430ba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCLayer(object):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.W = glorot_init(input_dim, output_dim)\n",
    "        self.W_grad = np.zeros(self.W.shape)\n",
    "        self.b = np.ones((output_dim, 1))\n",
    "        self.b_grad = np.zeros(self.b.shape)\n",
    "        \n",
    "        \n",
    "    def relu(self, x):\n",
    "        return np.piecewise(x, [x <= 0, x > 0], [0, x])\n",
    "    \n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        return (x > 0) * 1\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    inputs:\n",
    "    G (nx.Graph)   Normalized Laplacian matrix for a static graph.\n",
    "                   Dimensions: N x N where N is the number of nodes.\n",
    "    x (np.ndarray) Embedding matrix\n",
    "                   Dimensions: N x F where F is the number of features.\n",
    "    '''\n",
    "    def __call__(self, G, x):\n",
    "        # (nodes x nodes), (nodes x features), so need to transpose\n",
    "        # before taking linear combination\n",
    "        self.i = x # (nxf)\n",
    "        self.X = (G @ x).T # (n,n) x (n,f) -> (n,f).T -> (f,n)\n",
    "        self.z = self.W @ self.X + self.b # (h,f) x (f,n) + (h,1) -> (h,n)\n",
    "        self.a = self.relu(self.z) # (h,n), where n is number of samples/nodes\n",
    "        \n",
    "        # transpose so can multiply by adjacency matrix in next layer\n",
    "        return self.a.T # (n,h)\n",
    "    \n",
    "    \n",
    "    def backward(self, G, error):\n",
    "        samples = self.X.shape[0] # batch size\n",
    "        #should this be self.X or self.i?\n",
    "        self.W_grad += error @ self.X.T # (h,n) x (n,f) -> (h,f) which matches W.shape \n",
    "        self.b_grad += error # (h,n)\n",
    "        return self.W.T @ error * self.relu_derivative(self.z)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26912c5e",
   "metadata": {},
   "source": [
    "### Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d0c805",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(object):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.W = np.zeros((self.input_dim, self.output_dim))\n",
    "        self.W_grad = np.zeros(self.W.shape)\n",
    "        self.b = np.ones((self.output_dim, 1))\n",
    "        self.b_grad = np.zeros(self.b.shape)\n",
    "        \n",
    "        \n",
    "    def relu(self, x):\n",
    "        return np.piecewise(x, [x <= 0, x > 0], [0, x])\n",
    "    \n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        return (x > 0) * 1\n",
    "    \n",
    "\n",
    "    '''\n",
    "    inputs:\n",
    "    x (np.ndarray) Inputs to this layer\n",
    "    \n",
    "    outputs:\n",
    "    a (np.ndarray) Output activations\n",
    "    '''\n",
    "    def __call__(self, x):\n",
    "        self.x = x # (f,n)\n",
    "        self.z = self.W @ x + self.b # (h,f) x (f,n) + (h,1)\n",
    "        self.a = self.relu(self.z)   # (h,n)\n",
    "        return self.a # (h,n)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    inputs:\n",
    "    error (np.ndarray) Error signal of shape (W.out_dim, batch_size) from subsequent layer\n",
    "    \n",
    "    outputs:\n",
    "    \n",
    "    '''\n",
    "    def backward(self, error):\n",
    "        self.W_grad += error @ self.x.T # (h,n) x (n,f)\n",
    "        self.b_grad += error            # (h,n)\n",
    "        return self.W.T @ error * self.relu_derivative(self.z) # (f,h) x (h,n) * (h,n)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214eea96",
   "metadata": {},
   "source": [
    "### Graph Convolutional Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33583259",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(object):\n",
    "    def __init__(self, graph):\n",
    "        self.G = graph\n",
    "        self.nodes = self.G.shape[0]\n",
    "        self.embedding = np.eye(self.nodes)\n",
    "        self.l0 = GCLayer(self.nodes, 16)\n",
    "        self.l1 = Linear(16, 2)\n",
    "        self.parameters = [self.l0, self.l1]\n",
    "        \n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return softmax(self.l1(self.l0(self.G, x)))\n",
    "    \n",
    "    \n",
    "    def backward(self, x):\n",
    "        self.l0.backward(self.G, self.l1.backward(self.G, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63957da8",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea88fb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_ent(pred, labels):\n",
    "    return -np.log(pred)[np.arange(pred.shape[0]), np.argmax(labels, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d397de",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6860b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss, epochs, features, labels, opt):\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        epoch_loss = loss(model(features), labels).mean()\n",
    "            \n",
    "        model.backward()\n",
    "        opt.step()\n",
    "        opt.zero_gradients()\n",
    "        \n",
    "        print(f'epoch {e} loss: {epoch_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5d7e74",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257fd958",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4badfa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.eye(G.number_of_nodes())\n",
    "model = GCN(A_hat)\n",
    "opt = GradientDescent(model.parameters, lr)\n",
    "train(model, cross_ent, epochs, features, labels, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ca75c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
